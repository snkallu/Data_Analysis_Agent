{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c77ddd02-ef29-43e2-9407-f3289a4a00d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Analyst Assistant Agent\n",
    "\n",
    "## ðŸŽ¯ Project Goal\n",
    "Build an intelligent agent that supports data analysts by:\n",
    "* **Answering data requests** with 100% accurate results\n",
    "* **Querying source files** directly from storage containers\n",
    "* **Breaking down queries** into clear, understandable steps\n",
    "* **Generating KPIs and metrics** for management reports\n",
    "* **Validating results** to ensure accuracy and reliability\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ‘¥ Target Users\n",
    "* **Data Analysts** - Daily data queries and analysis\n",
    "* **Upper Management** - KPIs, metrics, and executive reports\n",
    "* **Data Engineers** - Supporting infrastructure and data quality\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—ï¸ Architecture Components\n",
    "\n",
    "### 1. **File Discovery Engine**\n",
    "   * Scans storage containers (Volumes, DBFS, cloud storage)\n",
    "   * Detects file formats (CSV, Parquet, JSON, Delta)\n",
    "   * Builds searchable data catalog\n",
    "   * Infers schemas automatically\n",
    "\n",
    "### 2. **Natural Language Query Parser**\n",
    "   * Understands analyst requests in plain English\n",
    "   * Extracts intent (metrics, filters, time ranges)\n",
    "   * Maps to available data sources\n",
    "   * Handles ambiguity with clarifying questions\n",
    "\n",
    "### 3. **SQL Generation Engine**\n",
    "   * Converts requests to optimized SQL\n",
    "   * Supports complex aggregations and joins\n",
    "   * Applies best practices (predicate pushdown, column pruning)\n",
    "   * Generates efficient queries for large datasets\n",
    "\n",
    "### 4. **Query Explanation System**\n",
    "   * Breaks down SQL into plain English steps\n",
    "   * Shows data sources and transformations\n",
    "   * Visualizes query logic flow\n",
    "   * Explains calculations and business logic\n",
    "\n",
    "### 5. **Execution & Validation Layer**\n",
    "   * Executes queries with error handling\n",
    "   * Validates results (null checks, range validation)\n",
    "   * Detects anomalies and outliers\n",
    "   * Provides confidence scores\n",
    "\n",
    "### 6. **Result Formatting & Reporting**\n",
    "   * Formats output for readability\n",
    "   * Generates visualizations\n",
    "   * Creates exportable reports\n",
    "   * Supports multiple output formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d54d2c77-337e-4cb9-856f-974ffb69370a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to best_selling_books volume in Unity Catalog\n",
    "volume_path = \"/Volumes/prep_databricks/default/best_selling_books\"\n",
    "\n",
    "print(\"ðŸ“š Best Selling Books Data Analysis\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“‚ Volume Path: {volume_path}\\n\")\n",
    "\n",
    "# List all files\n",
    "files = dbutils.fs.ls(volume_path)\n",
    "print(f\"ðŸ“„ Available files ({len(files)}):\")\n",
    "for file in files:\n",
    "    size_mb = file.size / (1024*1024)\n",
    "    print(f\"   â€¢ {file.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(\"\\nâœ… Connected to Unity Catalog volume successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f68602b-7653-4da6-903b-812baa285d1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports and Data Classes"
    }
   },
   "outputs": [],
   "source": [
    "# Read best selling books data from Unity Catalog volume\n",
    "print(\"ðŸ“Š Reading data files...\\n\")\n",
    "\n",
    "# Read individual year files\n",
    "df_2023 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(f\"{volume_path}/best sellin books 2023.csv\")\n",
    "df_2024 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(f\"{volume_path}/best sellin books 2024.csv\")\n",
    "df_2025 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(f\"{volume_path}/best sellin books 2025.csv\")\n",
    "\n",
    "# Read total/combined file\n",
    "df_total = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(f\"{volume_path}/best sellin books total.csv\")\n",
    "\n",
    "print(\"âœ… Data loaded successfully!\\n\")\n",
    "print(\"Available DataFrames:\")\n",
    "print(f\"   â€¢ df_2023: {df_2023.count():,} rows, {len(df_2023.columns)} columns\")\n",
    "print(f\"   â€¢ df_2024: {df_2024.count():,} rows, {len(df_2024.columns)} columns\")\n",
    "print(f\"   â€¢ df_2025: {df_2025.count():,} rows, {len(df_2025.columns)} columns\")\n",
    "print(f\"   â€¢ df_total: {df_total.count():,} rows, {len(df_total.columns)} columns\")\n",
    "\n",
    "print(\"\\nðŸ” Schema (df_total):\")\n",
    "df_total.printSchema()\n",
    "\n",
    "print(\"\\nðŸ“ Sample data (first 5 rows):\")\n",
    "display(df_total.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dc6d54d-2cee-4e73-ab90-a89546398896",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Simple Data Analysis Agent"
    }
   },
   "outputs": [],
   "source": [
    "# Simple Data Analysis Agent for Best Selling Books\n",
    "from pyspark.sql.functions import col, count, avg, sum as spark_sum, desc, round as spark_round, when\n",
    "\n",
    "class BooksAnalysisAgent:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.df_name = \"Best Selling Books\"\n",
    "        \n",
    "    def analyze_request(self, request):\n",
    "        \"\"\"\n",
    "        Analyze natural language requests about the books data.\n",
    "        \"\"\"\n",
    "        request_lower = request.lower()\n",
    "        \n",
    "        print(f\"\\nðŸ¤– Agent analyzing: '{request}'\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Top rated books\n",
    "        if \"top rated\" in request_lower or \"highest rated\" in request_lower:\n",
    "            return self._top_rated_books()\n",
    "        \n",
    "        # Most reviewed\n",
    "        elif \"most reviewed\" in request_lower or \"popular\" in request_lower:\n",
    "            return self._most_reviewed_books()\n",
    "        \n",
    "        # Genre analysis\n",
    "        elif \"genre\" in request_lower:\n",
    "            return self._genre_analysis()\n",
    "        \n",
    "        # Price analysis\n",
    "        elif \"price\" in request_lower or \"expensive\" in request_lower or \"cheap\" in request_lower:\n",
    "            return self._price_analysis()\n",
    "        \n",
    "        # Author analysis\n",
    "        elif \"author\" in request_lower:\n",
    "            return self._author_analysis()\n",
    "        \n",
    "        # Year trends\n",
    "        elif \"year\" in request_lower or \"trend\" in request_lower:\n",
    "            return self._year_trends()\n",
    "        \n",
    "        # Summary\n",
    "        elif \"summary\" in request_lower or \"overview\" in request_lower:\n",
    "            return self._data_summary()\n",
    "        \n",
    "        else:\n",
    "            print(\"\\nâ“ I can help you analyze:\")\n",
    "            print(\"  â€¢ Top rated books\")\n",
    "            print(\"  â€¢ Most reviewed books\")\n",
    "            print(\"  â€¢ Genre analysis\")\n",
    "            print(\"  â€¢ Price analysis\")\n",
    "            print(\"  â€¢ Author analysis\")\n",
    "            print(\"  â€¢ Year trends\")\n",
    "            print(\"  â€¢ Data summary\")\n",
    "            return None\n",
    "    \n",
    "    def _top_rated_books(self, limit=10):\n",
    "        print(\"\\nðŸ“Š Analysis: Top Rated Books\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Clean rating column and convert to numeric\n",
    "        result = (self.df\n",
    "                 .withColumn(\"rating_numeric\", \n",
    "                            col(\"Rating\").substr(1, 3).cast(\"double\"))\n",
    "                 .orderBy(desc(\"rating_numeric\"), desc(\"reviews count\"))\n",
    "                 .select(\"Book name\", \"Author\", \"Rating\", \"reviews count\", \"Genre\")\n",
    "                 .limit(limit))\n",
    "        \n",
    "        print(f\"\\nâœ… Found top {limit} highest rated books:\")\n",
    "        display(result)\n",
    "        return result\n",
    "    \n",
    "    def _most_reviewed_books(self, limit=10):\n",
    "        print(\"\\nðŸ“Š Analysis: Most Reviewed Books\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        result = (self.df\n",
    "                 .orderBy(desc(\"reviews count\"))\n",
    "                 .select(\"Book name\", \"Author\", \"Rating\", \"reviews count\", \"Genre\")\n",
    "                 .limit(limit))\n",
    "        \n",
    "        print(f\"\\nâœ… Found top {limit} most reviewed books:\")\n",
    "        display(result)\n",
    "        return result\n",
    "    \n",
    "    def _genre_analysis(self):\n",
    "        print(\"\\nðŸ“Š Analysis: Books by Genre\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        result = (self.df\n",
    "                 .groupBy(\"Genre\")\n",
    "                 .agg(\n",
    "                     count(\"*\").alias(\"book_count\"),\n",
    "                     avg(\"reviews count\").alias(\"avg_reviews\")\n",
    "                 )\n",
    "                 .orderBy(desc(\"book_count\")))\n",
    "        \n",
    "        print(\"\\nâœ… Genre breakdown:\")\n",
    "        display(result)\n",
    "        return result\n",
    "    \n",
    "    def _price_analysis(self):\n",
    "        print(\"\\nðŸ“Š Analysis: Price Distribution\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Clean price column\n",
    "        result = (self.df\n",
    "                 .withColumn(\"price_numeric\", \n",
    "                            col(\"price\").substr(2, 10).cast(\"double\"))\n",
    "                 .filter(col(\"price_numeric\").isNotNull())\n",
    "                 .groupBy(\"form\")\n",
    "                 .agg(\n",
    "                     count(\"*\").alias(\"count\"),\n",
    "                     spark_round(avg(\"price_numeric\"), 2).alias(\"avg_price\"),\n",
    "                     spark_round(spark_sum(\"price_numeric\") / spark_sum(\"reviews count\") * 1000, 4).alias(\"price_per_1k_reviews\")\n",
    "                 )\n",
    "                 .orderBy(desc(\"count\")))\n",
    "        \n",
    "        print(\"\\nâœ… Price analysis by format:\")\n",
    "        display(result)\n",
    "        return result\n",
    "    \n",
    "    def _author_analysis(self, limit=10):\n",
    "        print(\"\\nðŸ“Š Analysis: Top Authors\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        result = (self.df\n",
    "                 .groupBy(\"Author\")\n",
    "                 .agg(\n",
    "                     count(\"*\").alias(\"book_count\"),\n",
    "                     spark_sum(\"reviews count\").alias(\"total_reviews\")\n",
    "                 )\n",
    "                 .orderBy(desc(\"book_count\"), desc(\"total_reviews\"))\n",
    "                 .limit(limit))\n",
    "        \n",
    "        print(f\"\\nâœ… Top {limit} authors by number of books:\")\n",
    "        display(result)\n",
    "        return result\n",
    "    \n",
    "    def _year_trends(self):\n",
    "        print(\"\\nðŸ“Š Analysis: Year-over-Year Trends\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Count books that appeared in each year\n",
    "        result = self.df.select(\n",
    "            spark_sum(when(col(\"id_2023\").isNotNull(), 1).otherwise(0)).alias(\"books_in_2023\"),\n",
    "            spark_sum(when(col(\"id_2024\").isNotNull(), 1).otherwise(0)).alias(\"books_in_2024\"),\n",
    "            spark_sum(when(col(\"id_2025\").isNotNull(), 1).otherwise(0)).alias(\"books_in_2025\")\n",
    "        )\n",
    "        \n",
    "        print(\"\\nâœ… Books appearing in each year's top 100:\")\n",
    "        display(result)\n",
    "        \n",
    "        # Books that appeared in all years\n",
    "        all_years = (self.df\n",
    "                    .filter(col(\"id_2023\").isNotNull() & \n",
    "                           col(\"id_2024\").isNotNull() & \n",
    "                           col(\"id_2025\").isNotNull())\n",
    "                    .select(\"Book name\", \"Author\", \"id_2023\", \"id_2024\", \"id_2025\"))\n",
    "        \n",
    "        print(\"\\nâœ… Books that appeared in ALL three years:\")\n",
    "        display(all_years)\n",
    "        return result\n",
    "    \n",
    "    def _data_summary(self):\n",
    "        print(\"\\nðŸ“Š Analysis: Data Summary\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        total_books = self.df.count()\n",
    "        total_reviews = self.df.agg(spark_sum(\"reviews count\")).collect()[0][0]\n",
    "        unique_authors = self.df.select(\"Author\").distinct().count()\n",
    "        unique_genres = self.df.select(\"Genre\").distinct().count()\n",
    "        \n",
    "        print(f\"\\nâœ… Dataset Overview:\")\n",
    "        print(f\"  ðŸ“š Total unique books: {total_books:,}\")\n",
    "        print(f\"  â­ Total reviews: {total_reviews:,}\")\n",
    "        print(f\"  âœï¸  Unique authors: {unique_authors:,}\")\n",
    "        print(f\"  ðŸŽ­ Unique genres: {unique_genres:,}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\nðŸ“– Sample data:\")\n",
    "        display(self.df.limit(5))\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "# Initialize the agent with our data\n",
    "agent = BooksAnalysisAgent(df_total)\n",
    "\n",
    "print(\"\\nâœ… Books Analysis Agent initialized!\")\n",
    "print(\"\\nTry asking questions like:\")\n",
    "print(\"  â€¢ agent.analyze_request('Show me the top rated books')\")\n",
    "print(\"  â€¢ agent.analyze_request('What are the most reviewed books?')\")\n",
    "print(\"  â€¢ agent.analyze_request('Analyze by genre')\")\n",
    "print(\"  â€¢ agent.analyze_request('Show me price analysis')\")\n",
    "print(\"  â€¢ agent.analyze_request('Who are the top authors?')\")\n",
    "print(\"  â€¢ agent.analyze_request('Show year trends')\")\n",
    "print(\"  â€¢ agent.analyze_request('Give me a summary')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb1f059-92b5-44f7-8c09-f1b3dc3994ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ask the agent a question\n",
    "agent.analyze_request(\"Show me the top genre in books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c386a2-660f-42b7-83a8-b424401b1226",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run Agent Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Let's run the agent with different analysis requests\n",
    "\n",
    "# Analysis 1: Data Summary\n",
    "agent.analyze_request(\"Give me a summary of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b1c37e-049f-4e63-a1ba-a5ea942b2c54",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Top Rated Books Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Analysis 2: Top Rated Books\n",
    "agent.analyze_request(\"Show me the top rated books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "980c1b07-f74f-46e6-9588-935a04462ba8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Genre Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Analysis 3: Genre Breakdown\n",
    "agent.analyze_request(\"Analyze by genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e772467-b61c-45da-b379-bc0a161b99c9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Year Trends Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Analysis 4: Year-over-Year Trends\n",
    "agent.analyze_request(\"Show year trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b013cb4-c758-4d4d-999f-0305590d5aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ”„ Agent Workflow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    DATA ANALYST REQUEST                         â”‚\n",
    "â”‚  \"Show me total revenue by region for last quarter\"             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 1: UNDERSTAND REQUEST                                     â”‚\n",
    "â”‚  â€¢ Parse natural language                                       â”‚\n",
    "â”‚  â€¢ Extract: metric=revenue, dimension=region, time=last_quarter â”‚\n",
    "â”‚  â€¢ Identify required data sources                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 2: DISCOVER DATA SOURCES                                  â”‚\n",
    "â”‚  â€¢ Scan storage containers                                      â”‚\n",
    "â”‚  â€¢ Find relevant files: sales_data.parquet, regions.csv         â”‚\n",
    "â”‚  â€¢ Load schemas and metadata                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 3: GENERATE SQL QUERY                                     â”‚\n",
    "â”‚  â€¢ Build optimized SQL                                          â”‚\n",
    "â”‚  â€¢ Apply filters (date range)                                   â”‚\n",
    "â”‚  â€¢ Add aggregations (SUM, GROUP BY)                             â”‚\n",
    "â”‚  â€¢ Optimize for performance                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 4: EXPLAIN QUERY (Before Execution)                       â”‚\n",
    "â”‚  âœ“ Data Source: /volumes/sales/data/sales_2024.parquet         â”‚\n",
    "â”‚  âœ“ Filter: date >= '2024-10-01' AND date <= '2024-12-31'       â”‚\n",
    "â”‚  âœ“ Calculation: SUM(amount) AS total_revenue                    â”‚\n",
    "â”‚  âœ“ Grouping: BY region                                          â”‚\n",
    "â”‚  âœ“ Expected rows: ~5 regions                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 5: EXECUTE QUERY                                          â”‚\n",
    "â”‚  â€¢ Run SQL against data files                                   â”‚\n",
    "â”‚  â€¢ Monitor execution time                                       â”‚\n",
    "â”‚  â€¢ Capture any errors                                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 6: VALIDATE RESULTS                                       â”‚\n",
    "â”‚  âœ“ Row count check: 5 rows (expected ~5) âœ“                     â”‚\n",
    "â”‚  âœ“ Null check: No nulls in revenue âœ“                           â”‚\n",
    "â”‚  âœ“ Range check: Revenue values reasonable âœ“                    â”‚\n",
    "â”‚  âœ“ Confidence score: 98%                                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STEP 7: FORMAT & PRESENT RESULTS                               â”‚\n",
    "â”‚  ðŸ“Š Total Revenue by Region (Q4 2024)                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚\n",
    "â”‚  â”‚ Region       â”‚ Total Revenue   â”‚                            â”‚\n",
    "â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                            â”‚\n",
    "â”‚  â”‚ North        â”‚ $1,234,567      â”‚                            â”‚\n",
    "â”‚  â”‚ South        â”‚ $987,654        â”‚                            â”‚\n",
    "â”‚  â”‚ East         â”‚ $1,456,789      â”‚                            â”‚\n",
    "â”‚  â”‚ West         â”‚ $1,098,765      â”‚                            â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚\n",
    "â”‚  âœ… Results validated with 98% confidence                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ff5ad0-cfff-453b-8581-4a0058da1f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ¨ Key Features\n",
    "\n",
    "### ðŸŽ¯ Accuracy Guarantees\n",
    "* **Schema validation** - Verify data types and structure\n",
    "* **Result validation** - Check for nulls, outliers, anomalies\n",
    "* **Confidence scoring** - Rate result reliability (0-100%)\n",
    "* **Audit trail** - Log all queries and results\n",
    "* **Data lineage** - Track data source to result\n",
    "\n",
    "### ðŸ“ Query Explanation\n",
    "* **Plain English breakdown** - No SQL jargon\n",
    "* **Step-by-step logic** - Show transformation flow\n",
    "* **Data source transparency** - Which files are used\n",
    "* **Calculation details** - How metrics are computed\n",
    "* **Visual query plans** - Diagram query execution\n",
    "\n",
    "### ðŸ—‚ï¸ Data Source Management\n",
    "* **Auto-discovery** - Find files in storage containers\n",
    "* **Format detection** - CSV, Parquet, JSON, Delta, Avro\n",
    "* **Schema inference** - Automatic column type detection\n",
    "* **Metadata catalog** - Searchable data inventory\n",
    "* **Version tracking** - Handle schema evolution\n",
    "\n",
    "### ðŸ“Š KPI & Metrics Library\n",
    "* **Pre-built metrics** - Revenue, growth, conversion, churn\n",
    "* **Custom definitions** - Define new KPIs\n",
    "* **Time intelligence** - YoY, MoM, QoQ comparisons\n",
    "* **Dimensional analysis** - Slice by region, product, customer\n",
    "* **Trend analysis** - Historical patterns\n",
    "\n",
    "### ðŸš€ Performance Optimization\n",
    "* **Query optimization** - Predicate pushdown, column pruning\n",
    "* **Caching** - Reuse results for similar queries\n",
    "* **Incremental processing** - Only query new data\n",
    "* **Parallel execution** - Leverage Spark parallelism\n",
    "* **Resource management** - Efficient memory usage\n",
    "\n",
    "### ðŸ”’ Security & Governance\n",
    "* **Access control** - Respect Unity Catalog permissions\n",
    "* **Data masking** - Hide sensitive fields\n",
    "* **Audit logging** - Track all data access\n",
    "* **Compliance** - GDPR, HIPAA support\n",
    "* **Data quality** - Flag issues and anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe8222d2-0cce-4014-98ad-1d13fee10f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ’¼ Example Use Cases\n",
    "\n",
    "### Use Case 1: Daily Sales Report\n",
    "**Analyst Request:**\n",
    "> \"Show me yesterday's sales by product category\"\n",
    "\n",
    "**Agent Actions:**\n",
    "1. Identifies date filter: yesterday\n",
    "2. Finds sales data files\n",
    "3. Generates SQL with date filter and GROUP BY\n",
    "4. Explains: \"Querying sales_2024.parquet, filtering for 2024-02-09, summing sales by category\"\n",
    "5. Validates: 12 categories found, no nulls\n",
    "6. Returns formatted table with totals\n",
    "\n",
    "---\n",
    "\n",
    "### Use Case 2: Executive KPI Dashboard\n",
    "**Management Request:**\n",
    "> \"What's our month-over-month revenue growth?\"\n",
    "\n",
    "**Agent Actions:**\n",
    "1. Recognizes KPI: MoM revenue growth\n",
    "2. Calculates current month and previous month revenue\n",
    "3. Computes growth percentage\n",
    "4. Explains: \"Comparing Jan 2026 ($X) vs Dec 2025 ($Y), growth = Z%\"\n",
    "5. Validates: Both months have complete data\n",
    "6. Returns growth metric with confidence score\n",
    "\n",
    "---\n",
    "\n",
    "### Use Case 3: Ad-hoc Analysis\n",
    "**Analyst Request:**\n",
    "> \"Which customers in the West region spent more than $10,000 last quarter?\"\n",
    "\n",
    "**Agent Actions:**\n",
    "1. Parses filters: region=West, amount>10000, time=last_quarter\n",
    "2. Finds customer and transaction files\n",
    "3. Generates JOIN query\n",
    "4. Explains: \"Joining customers.csv with transactions.parquet, filtering by region and amount\"\n",
    "5. Validates: 47 customers found, all amounts > $10,000\n",
    "6. Returns customer list with spend amounts\n",
    "\n",
    "---\n",
    "\n",
    "### Use Case 4: Data Quality Check\n",
    "**Analyst Request:**\n",
    "> \"Are there any missing values in our customer email addresses?\"\n",
    "\n",
    "**Agent Actions:**\n",
    "1. Identifies data quality query\n",
    "2. Finds customer data file\n",
    "3. Generates null check query\n",
    "4. Explains: \"Counting NULL and empty strings in email column\"\n",
    "5. Validates: Found 23 missing emails out of 10,000 records\n",
    "6. Returns count and percentage, flags data quality issue\n",
    "\n",
    "---\n",
    "\n",
    "### Use Case 5: Trend Analysis\n",
    "**Management Request:**\n",
    "> \"Show me weekly active users for the past 6 months\"\n",
    "\n",
    "**Agent Actions:**\n",
    "1. Recognizes time-series analysis\n",
    "2. Finds user activity logs\n",
    "3. Generates weekly aggregation query\n",
    "4. Explains: \"Counting distinct users per week from Aug 2025 to Jan 2026\"\n",
    "5. Validates: 26 weeks of data, no gaps\n",
    "6. Returns time-series data with trend visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12a2709-f04e-4a48-a0e4-4c1c804a95ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Agent Core - Imports and Data Classes"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, count, sum as spark_sum, avg, min as spark_min, max as spark_max\n",
    "from pyspark.sql.functions import current_timestamp, lit, when, trim, lower, upper, to_date, datediff\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class DataSource:\n",
    "    \"\"\"Represents a discovered data source.\"\"\"\n",
    "    path: str\n",
    "    format: str\n",
    "    schema: StructType\n",
    "    row_count: Optional[int] = None\n",
    "    size_bytes: Optional[int] = None\n",
    "    last_modified: Optional[str] = None\n",
    "    columns: List[str] = field(default_factory=list)\n",
    "    \n",
    "@dataclass\n",
    "class QueryRequest:\n",
    "    \"\"\"Represents an analyst's data request.\"\"\"\n",
    "    original_text: str\n",
    "    intent: str\n",
    "    metrics: List[str] = field(default_factory=list)\n",
    "    dimensions: List[str] = field(default_factory=list)\n",
    "    filters: Dict[str, Any] = field(default_factory=dict)\n",
    "    time_range: Optional[Dict[str, str]] = None\n",
    "    \n",
    "@dataclass\n",
    "class QueryResult:\n",
    "    \"\"\"Represents query execution results with metadata.\"\"\"\n",
    "    data: DataFrame\n",
    "    sql_query: str\n",
    "    explanation: List[str]\n",
    "    execution_time_ms: float\n",
    "    row_count: int\n",
    "    validation_score: float\n",
    "    warnings: List[str] = field(default_factory=list)\n",
    "    data_sources: List[str] = field(default_factory=list)\n",
    "\n",
    "print(\"âœ… Data classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754b78c9-7096-460f-8016-93bda2f2b6a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DataAnalystAgent Class - Initialization"
    }
   },
   "outputs": [],
   "source": [
    "class DataAnalystAgent:\n",
    "    \"\"\"\n",
    "    AI Agent for supporting data analysts with accurate, explainable query results.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spark: SparkSession, storage_paths: List[str] = None):\n",
    "        self.spark = spark\n",
    "        self.storage_paths = storage_paths or []\n",
    "        self.data_catalog: Dict[str, DataSource] = {}\n",
    "        self.query_history: List[QueryResult] = []\n",
    "        self.kpi_definitions: Dict[str, Dict] = {}\n",
    "        \n",
    "        # Initialize KPI library\n",
    "        self._initialize_kpi_library()\n",
    "        \n",
    "        print(\"ðŸ¤– Data Analyst Agent initialized\")\n",
    "        print(f\"   Spark version: {spark.version}\")\n",
    "        print(f\"   Storage paths: {len(self.storage_paths)} configured\")\n",
    "    \n",
    "    def _initialize_kpi_library(self):\n",
    "        \"\"\"Initialize common KPI definitions.\"\"\"\n",
    "        self.kpi_definitions = {\n",
    "            \"revenue\": {\n",
    "                \"metric\": \"SUM(amount)\",\n",
    "                \"description\": \"Total revenue\",\n",
    "                \"required_columns\": [\"amount\"]\n",
    "            },\n",
    "            \"average_order_value\": {\n",
    "                \"metric\": \"AVG(amount)\",\n",
    "                \"description\": \"Average order value\",\n",
    "                \"required_columns\": [\"amount\"]\n",
    "            },\n",
    "            \"customer_count\": {\n",
    "                \"metric\": \"COUNT(DISTINCT customer_id)\",\n",
    "                \"description\": \"Unique customer count\",\n",
    "                \"required_columns\": [\"customer_id\"]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def add_storage_path(self, path: str):\n",
    "        \"\"\"Add a storage path to scan for data files.\"\"\"\n",
    "        if path not in self.storage_paths:\n",
    "            self.storage_paths.append(path)\n",
    "            print(f\"âœ… Added storage path: {path}\")\n",
    "    \n",
    "    def get_catalog_summary(self) -> Dict:\n",
    "        \"\"\"Get summary of discovered data sources.\"\"\"\n",
    "        return {\n",
    "            \"total_sources\": len(self.data_catalog),\n",
    "            \"formats\": list(set(ds.format for ds in self.data_catalog.values())),\n",
    "            \"total_columns\": sum(len(ds.columns) for ds in self.data_catalog.values()),\n",
    "            \"sources\": list(self.data_catalog.keys())\n",
    "        }\n",
    "\n",
    "print(\"âœ… DataAnalystAgent class initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3bfc140-cc5a-4155-8cb3-7ec6c7238e37",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "File Discovery Methods"
    }
   },
   "outputs": [],
   "source": [
    "def discover_files(self, path: str = None, pattern: str = \"*\") -> List[DataSource]:\n",
    "    \"\"\"\n",
    "    Discover data files in storage containers.\n",
    "    \n",
    "    Args:\n",
    "        path: Specific path to scan (uses configured paths if None)\n",
    "        pattern: File pattern to match (e.g., '*.parquet', 'sales_*')\n",
    "        \n",
    "    Returns:\n",
    "        List of discovered DataSource objects\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” Discovering data files...\")\n",
    "    \n",
    "    paths_to_scan = [path] if path else self.storage_paths\n",
    "    discovered = []\n",
    "    \n",
    "    for scan_path in paths_to_scan:\n",
    "        try:\n",
    "            # List files using dbutils\n",
    "            try:\n",
    "                files = dbutils.fs.ls(scan_path)\n",
    "            except:\n",
    "                print(f\"   âš ï¸  Could not list directory: {scan_path}\")\n",
    "                continue\n",
    "            \n",
    "            for file_info in files:\n",
    "                file_path = file_info.path\n",
    "                file_name = file_info.name\n",
    "                \n",
    "                # Skip directories and hidden files\n",
    "                if file_info.isDir() or file_name.startswith('.'):\n",
    "                    continue\n",
    "                \n",
    "                # Detect file format\n",
    "                file_format = self._detect_format(file_name)\n",
    "                if not file_format:\n",
    "                    continue\n",
    "                \n",
    "                # Try to read schema\n",
    "                try:\n",
    "                    if file_format == \"delta\":\n",
    "                        df = self.spark.read.format(\"delta\").load(file_path)\n",
    "                    elif file_format == \"parquet\":\n",
    "                        df = self.spark.read.parquet(file_path)\n",
    "                    elif file_format == \"csv\":\n",
    "                        df = self.spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "                    elif file_format == \"json\":\n",
    "                        df = self.spark.read.json(file_path)\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    # Create DataSource object\n",
    "                    data_source = DataSource(\n",
    "                        path=file_path,\n",
    "                        format=file_format,\n",
    "                        schema=df.schema,\n",
    "                        columns=[field.name for field in df.schema.fields],\n",
    "                        size_bytes=file_info.size,\n",
    "                        last_modified=str(file_info.modificationTime)\n",
    "                    )\n",
    "                    \n",
    "                    # Add to catalog\n",
    "                    self.data_catalog[file_name] = data_source\n",
    "                    discovered.append(data_source)\n",
    "                    \n",
    "                    print(f\"   âœ… Found: {file_name} ({file_format}, {len(data_source.columns)} columns)\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   âŒ Error reading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error scanning {scan_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nâœ… Discovery complete: {len(discovered)} files found\")\n",
    "    return discovered\n",
    "\n",
    "def _detect_format(self, filename: str) -> Optional[str]:\n",
    "    \"\"\"Detect file format from filename.\"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    if filename_lower.endswith('.parquet'):\n",
    "        return 'parquet'\n",
    "    elif filename_lower.endswith('.csv'):\n",
    "        return 'csv'\n",
    "    elif filename_lower.endswith('.json'):\n",
    "        return 'json'\n",
    "    elif filename_lower.endswith('.delta') or '_delta_log' in filename_lower:\n",
    "        return 'delta'\n",
    "    return None\n",
    "\n",
    "# Add methods to class\n",
    "DataAnalystAgent.discover_files = discover_files\n",
    "DataAnalystAgent._detect_format = _detect_format\n",
    "\n",
    "print(\"âœ… File discovery methods added to DataAnalystAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a5d3396-0ca3-4cb2-9606-02662e3e7dbc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Request Understanding Methods"
    }
   },
   "outputs": [],
   "source": [
    "def understand_request(self, request_text: str) -> QueryRequest:\n",
    "    \"\"\"\n",
    "    Parse natural language request into structured QueryRequest.\n",
    "    \n",
    "    Args:\n",
    "        request_text: Analyst's request in natural language\n",
    "        \n",
    "    Returns:\n",
    "        QueryRequest object with parsed intent and parameters\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ§  Understanding request: '{request_text}'\")\n",
    "    \n",
    "    request_lower = request_text.lower()\n",
    "    \n",
    "    # Detect intent\n",
    "    intent = self._detect_intent(request_lower)\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics = self._extract_metrics(request_lower)\n",
    "    \n",
    "    # Extract dimensions\n",
    "    dimensions = self._extract_dimensions(request_lower)\n",
    "    \n",
    "    # Extract filters\n",
    "    filters = self._extract_filters(request_lower)\n",
    "    \n",
    "    # Extract time range\n",
    "    time_range = self._extract_time_range(request_lower)\n",
    "    \n",
    "    query_request = QueryRequest(\n",
    "        original_text=request_text,\n",
    "        intent=intent,\n",
    "        metrics=metrics,\n",
    "        dimensions=dimensions,\n",
    "        filters=filters,\n",
    "        time_range=time_range\n",
    "    )\n",
    "    \n",
    "    print(f\"   Intent: {intent}\")\n",
    "    print(f\"   Metrics: {metrics}\")\n",
    "    print(f\"   Dimensions: {dimensions}\")\n",
    "    if filters:\n",
    "        print(f\"   Filters: {filters}\")\n",
    "    if time_range:\n",
    "        print(f\"   Time range: {time_range}\")\n",
    "    \n",
    "    return query_request\n",
    "\n",
    "def _detect_intent(self, text: str) -> str:\n",
    "    \"\"\"Detect query intent from text.\"\"\"\n",
    "    if any(word in text for word in ['total', 'sum', 'count', 'average', 'avg']):\n",
    "        return 'aggregate'\n",
    "    elif any(word in text for word in ['growth', 'change', 'trend', 'over time']):\n",
    "        return 'trend'\n",
    "    elif any(word in text for word in ['kpi', 'metric', 'performance']):\n",
    "        return 'kpi'\n",
    "    elif any(word in text for word in ['where', 'filter', 'only', 'specific']):\n",
    "        return 'filter'\n",
    "    elif any(word in text for word in ['join', 'combine', 'merge', 'with']):\n",
    "        return 'join'\n",
    "    else:\n",
    "        return 'general'\n",
    "\n",
    "def _extract_metrics(self, text: str) -> List[str]:\n",
    "    \"\"\"Extract metrics from text.\"\"\"\n",
    "    metrics = []\n",
    "    metric_keywords = {\n",
    "        'revenue': ['revenue', 'sales', 'income'],\n",
    "        'count': ['count', 'number of', 'how many'],\n",
    "        'average': ['average', 'avg', 'mean'],\n",
    "        'total': ['total', 'sum'],\n",
    "        'max': ['maximum', 'max', 'highest'],\n",
    "        'min': ['minimum', 'min', 'lowest']\n",
    "    }\n",
    "    \n",
    "    for metric, keywords in metric_keywords.items():\n",
    "        if any(kw in text for kw in keywords):\n",
    "            metrics.append(metric)\n",
    "    \n",
    "    return metrics if metrics else ['count']\n",
    "\n",
    "def _extract_dimensions(self, text: str) -> List[str]:\n",
    "    \"\"\"Extract dimensions (group by columns) from text.\"\"\"\n",
    "    dimensions = []\n",
    "    \n",
    "    # Common dimensions\n",
    "    common_dims = ['region', 'category', 'product', 'customer', 'date', 'month', 'year', 'quarter']\n",
    "    \n",
    "    for dim in common_dims:\n",
    "        if dim in text:\n",
    "            dimensions.append(dim)\n",
    "    \n",
    "    return dimensions\n",
    "\n",
    "def _extract_filters(self, text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract filter conditions from text.\"\"\"\n",
    "    filters = {}\n",
    "    \n",
    "    # Extract comparison filters\n",
    "    amount_pattern = r'(more than|greater than|less than|at least|over|under)\\s+(\\$?[\\d,]+)'\n",
    "    matches = re.findall(amount_pattern, text)\n",
    "    if matches:\n",
    "        operator, value = matches[0]\n",
    "        value_clean = value.replace('$', '').replace(',', '')\n",
    "        if 'more than' in operator or 'greater than' in operator or 'over' in operator:\n",
    "            filters['amount'] = {'operator': '>', 'value': float(value_clean)}\n",
    "        elif 'less than' in operator or 'under' in operator:\n",
    "            filters['amount'] = {'operator': '<', 'value': float(value_clean)}\n",
    "    \n",
    "    # Extract region/category filters\n",
    "    if 'in the' in text or 'from' in text:\n",
    "        for word in text.split():\n",
    "            if word.capitalize() in ['North', 'South', 'East', 'West']:\n",
    "                filters['region'] = word.capitalize()\n",
    "    \n",
    "    return filters\n",
    "\n",
    "def _extract_time_range(self, text: str) -> Optional[Dict[str, str]]:\n",
    "    \"\"\"Extract time range from text.\"\"\"\n",
    "    today = datetime.now()\n",
    "    \n",
    "    if 'yesterday' in text:\n",
    "        date = today - timedelta(days=1)\n",
    "        return {'start': date.strftime('%Y-%m-%d'), 'end': date.strftime('%Y-%m-%d')}\n",
    "    elif 'last week' in text:\n",
    "        start = today - timedelta(days=7)\n",
    "        return {'start': start.strftime('%Y-%m-%d'), 'end': today.strftime('%Y-%m-%d')}\n",
    "    elif 'last month' in text:\n",
    "        start = today - timedelta(days=30)\n",
    "        return {'start': start.strftime('%Y-%m-%d'), 'end': today.strftime('%Y-%m-%d')}\n",
    "    elif 'last quarter' in text or 'q4' in text or 'quarter' in text:\n",
    "        start = today - timedelta(days=90)\n",
    "        return {'start': start.strftime('%Y-%m-%d'), 'end': today.strftime('%Y-%m-%d')}\n",
    "    elif 'last year' in text:\n",
    "        start = today - timedelta(days=365)\n",
    "        return {'start': start.strftime('%Y-%m-%d'), 'end': today.strftime('%Y-%m-%d')}\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Add methods to class\n",
    "DataAnalystAgent.understand_request = understand_request\n",
    "DataAnalystAgent._detect_intent = _detect_intent\n",
    "DataAnalystAgent._extract_metrics = _extract_metrics\n",
    "DataAnalystAgent._extract_dimensions = _extract_dimensions\n",
    "DataAnalystAgent._extract_filters = _extract_filters\n",
    "DataAnalystAgent._extract_time_range = _extract_time_range\n",
    "\n",
    "print(\"âœ… Request understanding methods added to DataAnalystAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9128337-7a17-4017-98a8-b62d1652de75",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query Generation Methods"
    }
   },
   "outputs": [],
   "source": [
    "def generate_query(self, request: QueryRequest, data_source: DataSource) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Generate SQL query from QueryRequest.\n",
    "    \n",
    "    Args:\n",
    "        request: Parsed query request\n",
    "        data_source: Data source to query\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (SQL query string, explanation steps)\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ› ï¸ Generating SQL query...\")\n",
    "    \n",
    "    explanation = []\n",
    "    explanation.append(f\"Data source: {data_source.path} ({data_source.format} format)\")\n",
    "    \n",
    "    # Build SELECT clause\n",
    "    select_parts = []\n",
    "    \n",
    "    # Add dimensions\n",
    "    for dim in request.dimensions:\n",
    "        if dim in data_source.columns:\n",
    "            select_parts.append(dim)\n",
    "            explanation.append(f\"Group results by: {dim}\")\n",
    "    \n",
    "    # Add metrics\n",
    "    for metric in request.metrics:\n",
    "        if metric == 'revenue' or metric == 'total':\n",
    "            if 'amount' in data_source.columns:\n",
    "                select_parts.append(\"SUM(amount) AS total_amount\")\n",
    "                explanation.append(\"Calculate: Total sum of amount column\")\n",
    "            elif 'revenue' in data_source.columns:\n",
    "                select_parts.append(\"SUM(revenue) AS total_revenue\")\n",
    "                explanation.append(\"Calculate: Total sum of revenue column\")\n",
    "        elif metric == 'count':\n",
    "            select_parts.append(\"COUNT(*) AS record_count\")\n",
    "            explanation.append(\"Calculate: Count of all records\")\n",
    "        elif metric == 'average':\n",
    "            if 'amount' in data_source.columns:\n",
    "                select_parts.append(\"AVG(amount) AS average_amount\")\n",
    "                explanation.append(\"Calculate: Average of amount column\")\n",
    "    \n",
    "    # Default to COUNT(*) if no metrics\n",
    "    if not any('AS' in part for part in select_parts):\n",
    "        select_parts.append(\"COUNT(*) AS record_count\")\n",
    "        explanation.append(\"Calculate: Count of all records\")\n",
    "    \n",
    "    select_clause = \"SELECT \" + \", \".join(select_parts)\n",
    "    \n",
    "    # Build FROM clause using read_files()\n",
    "    from_clause = f\"FROM read_files('{data_source.path}')\"\n",
    "    \n",
    "    # Build WHERE clause\n",
    "    where_conditions = []\n",
    "    \n",
    "    # Add time range filter\n",
    "    if request.time_range:\n",
    "        date_col = next((col for col in data_source.columns if 'date' in col.lower()), None)\n",
    "        if date_col:\n",
    "            where_conditions.append(f\"{date_col} >= '{request.time_range['start']}'\")\n",
    "            where_conditions.append(f\"{date_col} <= '{request.time_range['end']}'\")\n",
    "            explanation.append(f\"Filter: Date range from {request.time_range['start']} to {request.time_range['end']}\")\n",
    "    \n",
    "    # Add other filters\n",
    "    for col_name, filter_spec in request.filters.items():\n",
    "        if col_name in data_source.columns:\n",
    "            if isinstance(filter_spec, dict):\n",
    "                operator = filter_spec.get('operator', '=')\n",
    "                value = filter_spec.get('value')\n",
    "                where_conditions.append(f\"{col_name} {operator} {value}\")\n",
    "                explanation.append(f\"Filter: {col_name} {operator} {value}\")\n",
    "            else:\n",
    "                where_conditions.append(f\"{col_name} = '{filter_spec}'\")\n",
    "                explanation.append(f\"Filter: {col_name} = {filter_spec}\")\n",
    "    \n",
    "    where_clause = \"WHERE \" + \" AND \".join(where_conditions) if where_conditions else \"\"\n",
    "    \n",
    "    # Build GROUP BY clause\n",
    "    group_by_clause = \"\"\n",
    "    if request.dimensions:\n",
    "        group_by_cols = [dim for dim in request.dimensions if dim in data_source.columns]\n",
    "        if group_by_cols:\n",
    "            group_by_clause = \"GROUP BY \" + \", \".join(group_by_cols)\n",
    "    \n",
    "    # Build ORDER BY clause\n",
    "    order_by_clause = \"\"\n",
    "    if request.dimensions:\n",
    "        order_by_clause = f\"ORDER BY {request.dimensions[0]}\"\n",
    "    \n",
    "    # Combine all parts\n",
    "    query_parts = [select_clause, from_clause]\n",
    "    if where_clause:\n",
    "        query_parts.append(where_clause)\n",
    "    if group_by_clause:\n",
    "        query_parts.append(group_by_clause)\n",
    "    if order_by_clause:\n",
    "        query_parts.append(order_by_clause)\n",
    "    \n",
    "    sql_query = \"\\n\".join(query_parts)\n",
    "    \n",
    "    print(f\"   âœ… Query generated ({len(explanation)} explanation steps)\")\n",
    "    return sql_query, explanation\n",
    "\n",
    "# Add method to class\n",
    "DataAnalystAgent.generate_query = generate_query\n",
    "\n",
    "print(\"âœ… Query generation methods added to DataAnalystAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18a3d824-75ed-4189-aa3a-78236ad5b3eb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query Execution and Validation Methods"
    }
   },
   "outputs": [],
   "source": [
    "def execute_query(self, sql_query: str, explanation: List[str], data_sources: List[str]) -> QueryResult:\n",
    "    \"\"\"\n",
    "    Execute SQL query and validate results.\n",
    "    \n",
    "    Args:\n",
    "        sql_query: SQL query to execute\n",
    "        explanation: Query explanation steps\n",
    "        data_sources: List of data source paths used\n",
    "        \n",
    "    Returns:\n",
    "        QueryResult with data and validation metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\nâ–¶ï¸ Executing query...\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    warnings = []\n",
    "    \n",
    "    try:\n",
    "        # Execute query\n",
    "        result_df = self.spark.sql(sql_query)\n",
    "        \n",
    "        # Get row count\n",
    "        row_count = result_df.count()\n",
    "        \n",
    "        execution_time = (datetime.now() - start_time).total_seconds() * 1000\n",
    "        \n",
    "        print(f\"   âœ… Query executed in {execution_time:.0f}ms\")\n",
    "        print(f\"   âœ… Returned {row_count} rows\")\n",
    "        \n",
    "        # Validate results\n",
    "        validation_score, validation_warnings = self._validate_results(result_df, row_count)\n",
    "        warnings.extend(validation_warnings)\n",
    "        \n",
    "        return QueryResult(\n",
    "            data=result_df,\n",
    "            sql_query=sql_query,\n",
    "            explanation=explanation,\n",
    "            execution_time_ms=execution_time,\n",
    "            row_count=row_count,\n",
    "            validation_score=validation_score,\n",
    "            warnings=warnings,\n",
    "            data_sources=data_sources\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Query execution failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def _validate_results(self, df: DataFrame, row_count: int) -> Tuple[float, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate query results and return confidence score.\n",
    "    \n",
    "    Args:\n",
    "        df: Result DataFrame\n",
    "        row_count: Number of rows returned\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (validation score 0-100, list of warnings)\n",
    "    \"\"\"\n",
    "    score = 100.0\n",
    "    warnings = []\n",
    "    \n",
    "    # Check 1: Empty results\n",
    "    if row_count == 0:\n",
    "        score -= 50\n",
    "        warnings.append(\"âš ï¸  Query returned no results\")\n",
    "        return score, warnings\n",
    "    \n",
    "    # Check 2: Null values in results\n",
    "    for col_name in df.columns:\n",
    "        null_count = df.filter(df[col_name].isNull()).count()\n",
    "        if null_count > 0:\n",
    "            null_pct = (null_count / row_count) * 100\n",
    "            if null_pct > 50:\n",
    "                score -= 20\n",
    "                warnings.append(f\"âš ï¸  Column '{col_name}' has {null_pct:.1f}% null values\")\n",
    "            elif null_pct > 10:\n",
    "                score -= 5\n",
    "                warnings.append(f\"âš ï¸  Column '{col_name}' has {null_pct:.1f}% null values\")\n",
    "    \n",
    "    # Check 3: Reasonable row count\n",
    "    if row_count > 1000000:\n",
    "        score -= 10\n",
    "        warnings.append(f\"âš ï¸  Large result set ({row_count:,} rows) - consider adding filters\")\n",
    "    \n",
    "    # Check 4: Data type consistency for amount columns\n",
    "    for field in df.schema.fields:\n",
    "        if 'amount' in field.name.lower() or 'revenue' in field.name.lower():\n",
    "            # Check for negative values in amount columns\n",
    "            try:\n",
    "                negative_count = df.filter(df[field.name] < 0).count()\n",
    "                if negative_count > 0:\n",
    "                    score -= 10\n",
    "                    warnings.append(f\"âš ï¸  Column '{field.name}' has {negative_count} negative values\")\n",
    "            except:\n",
    "                pass  # Skip if comparison not supported\n",
    "    \n",
    "    return max(score, 0), warnings\n",
    "\n",
    "# Add methods to class\n",
    "DataAnalystAgent.execute_query = execute_query\n",
    "DataAnalystAgent._validate_results = _validate_results\n",
    "\n",
    "print(\"âœ… Query execution and validation methods added to DataAnalystAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf2de22e-704b-401d-b372-967a70a8ac9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Result Formatting and Explanation Methods"
    }
   },
   "outputs": [],
   "source": [
    "def explain_query(self, result: QueryResult) -> None:\n",
    "    \"\"\"\n",
    "    Print detailed query explanation in plain English.\n",
    "    \n",
    "    Args:\n",
    "        result: QueryResult object\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“ QUERY EXPLANATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nðŸ“„ What this query does:\")\n",
    "    for i, step in enumerate(result.explanation, 1):\n",
    "        print(f\"   {i}. {step}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Data Sources:\")\n",
    "    for source in result.data_sources:\n",
    "        print(f\"   â€¢ {source}\")\n",
    "    \n",
    "    print(f\"\\nâ±ï¸  Execution Time: {result.execution_time_ms:.0f}ms\")\n",
    "    print(f\"ðŸ“ˆ Rows Returned: {result.row_count:,}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Validation Score: {result.validation_score:.0f}%\")\n",
    "    if result.warnings:\n",
    "        print(\"\\nâš ï¸  Warnings:\")\n",
    "        for warning in result.warnings:\n",
    "            print(f\"   {warning}\")\n",
    "    else:\n",
    "        print(\"   No issues detected - results are reliable\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š SQL QUERY\")\n",
    "    print(\"=\"*80)\n",
    "    print(result.sql_query)\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def format_results(self, result: QueryResult, limit: int = 100) -> None:\n",
    "    \"\"\"\n",
    "    Display formatted query results.\n",
    "    \n",
    "    Args:\n",
    "        result: QueryResult object\n",
    "        limit: Maximum rows to display\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ“Š QUERY RESULTS ({result.row_count:,} total rows)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if result.row_count == 0:\n",
    "        print(\"âš ï¸  No results found\")\n",
    "        return\n",
    "    \n",
    "    # Display results\n",
    "    display(result.data.limit(limit))\n",
    "    \n",
    "    if result.row_count > limit:\n",
    "        print(f\"\\nðŸ“Œ Showing first {limit} of {result.row_count:,} rows\")\n",
    "\n",
    "# Add methods to class\n",
    "DataAnalystAgent.explain_query = explain_query\n",
    "DataAnalystAgent.format_results = format_results\n",
    "\n",
    "print(\"âœ… Result formatting methods added to DataAnalystAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dd3fe2d-ee1f-4403-bef8-3fb0847109c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main Workflow Method - answer_request"
    }
   },
   "outputs": [],
   "source": [
    "def answer_request(self, request_text: str, data_source_name: str = None) -> QueryResult:\n",
    "    \"\"\"\n",
    "    Complete workflow: understand request, generate query, execute, and explain.\n",
    "    \n",
    "    This is the main method analysts will use.\n",
    "    \n",
    "    Args:\n",
    "        request_text: Analyst's request in natural language\n",
    "        data_source_name: Specific data source to use (auto-detect if None)\n",
    "        \n",
    "    Returns:\n",
    "        QueryResult object with data and metadata\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"ðŸš€ \"*30)\n",
    "    print(\"ðŸš€ PROCESSING ANALYST REQUEST\")\n",
    "    print(\"ðŸš€ \"*30 + \"\\n\")\n",
    "    print(f\"ðŸ’¬ Request: '{request_text}'\\n\")\n",
    "    \n",
    "    # Step 1: Understand request\n",
    "    request = self.understand_request(request_text)\n",
    "    \n",
    "    # Step 2: Find appropriate data source\n",
    "    if data_source_name:\n",
    "        if data_source_name not in self.data_catalog:\n",
    "            raise ValueError(f\"Data source '{data_source_name}' not found in catalog\")\n",
    "        data_source = self.data_catalog[data_source_name]\n",
    "    else:\n",
    "        # Auto-select first available data source\n",
    "        if not self.data_catalog:\n",
    "            raise ValueError(\"No data sources available. Run discover_files() first.\")\n",
    "        data_source = list(self.data_catalog.values())[0]\n",
    "        print(f\"\\nðŸ’¾ Auto-selected data source: {list(self.data_catalog.keys())[0]}\")\n",
    "    \n",
    "    # Step 3: Generate query\n",
    "    sql_query, explanation = self.generate_query(request, data_source)\n",
    "    \n",
    "    # Step 4: Execute query\n",
    "    result = self.execute_query(sql_query, explanation, [data_source.path])\n",
    "    \n",
    "    # Step 5: Explain and display\n",
    "    self.explain_query(result)\n",
    "    self.format_results(result)\n",
    "    \n",
    "    # Save to history\n",
    "    self.query_history.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"âœ… \"*30)\n",
    "    print(\"âœ… REQUEST COMPLETED SUCCESSFULLY\")\n",
    "    print(\"âœ… \"*30 + \"\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Add method to class\n",
    "DataAnalystAgent.answer_request = answer_request\n",
    "\n",
    "print(\"âœ… Main workflow method added to DataAnalystAgent\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… DATA ANALYST AGENT MODULE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸŽ‰ The agent is ready to use!\")\n",
    "print(\"\\nQuick Start:\")\n",
    "print(\"  1. agent = DataAnalystAgent(spark, ['/path/to/data'])\")\n",
    "print(\"  2. agent.discover_files()\")\n",
    "print(\"  3. result = agent.answer_request('Show me total revenue by region')\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a488698-6c18-4ffb-a07d-2e20663f897f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Sample Sales Data"
    }
   },
   "outputs": [],
   "source": [
    "# Generate sample sales data for testing\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType, IntegerType\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "print(\"ðŸ“Š Generating sample sales data...\\n\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Generate dates for the past year\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2025, 2, 10)\n",
    "date_range = [(start_date + timedelta(days=x)).strftime('%Y-%m-%d') \n",
    "              for x in range((end_date - start_date).days)]\n",
    "\n",
    "# Sample data parameters\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "categories = ['Electronics', 'Clothing', 'Food', 'Home', 'Sports']\n",
    "products = {\n",
    "    'Electronics': ['Laptop', 'Phone', 'Tablet', 'Headphones'],\n",
    "    'Clothing': ['Shirt', 'Pants', 'Jacket', 'Shoes'],\n",
    "    'Food': ['Snacks', 'Beverages', 'Frozen', 'Fresh'],\n",
    "    'Home': ['Furniture', 'Decor', 'Kitchen', 'Bedding'],\n",
    "    'Sports': ['Equipment', 'Apparel', 'Footwear', 'Accessories']\n",
    "}\n",
    "\n",
    "# Generate 1000 sales records\n",
    "sales_data = []\n",
    "for i in range(1000):\n",
    "    date = random.choice(date_range)\n",
    "    region = random.choice(regions)\n",
    "    category = random.choice(categories)\n",
    "    product = random.choice(products[category])\n",
    "    amount = round(random.uniform(10, 5000), 2)\n",
    "    quantity = random.randint(1, 10)\n",
    "    customer_id = f\"CUST{random.randint(1000, 9999)}\"\n",
    "    \n",
    "    sales_data.append((\n",
    "        i + 1,\n",
    "        date,\n",
    "        region,\n",
    "        category,\n",
    "        product,\n",
    "        amount,\n",
    "        quantity,\n",
    "        customer_id\n",
    "    ))\n",
    "\n",
    "# Create DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), False),\n",
    "    StructField(\"date\", StringType(), False),\n",
    "    StructField(\"region\", StringType(), False),\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"product\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", StringType(), False)\n",
    "])\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, schema)\n",
    "\n",
    "print(f\"âœ… Generated {sales_df.count()} sales records\")\n",
    "print(f\"   Date range: {min(date_range)} to {max(date_range)}\")\n",
    "print(f\"   Regions: {', '.join(regions)}\")\n",
    "print(f\"   Categories: {', '.join(categories)}\")\n",
    "print(\"\\nðŸ“Š Sample data preview:\")\n",
    "display(sales_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c731547b-0fd4-4d19-ab54-2548116fedb3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save Sample Data to Storage"
    }
   },
   "outputs": [],
   "source": [
    "# Save sample data to DBFS for testing\n",
    "import os\n",
    "\n",
    "# Define storage path\n",
    "storage_path = \"/tmp/analyst_agent_demo/sales_data.parquet\"\n",
    "\n",
    "print(f\"ðŸ’¾ Saving sample data to: {storage_path}\\n\")\n",
    "\n",
    "# Save as Parquet\n",
    "sales_df.write.mode(\"overwrite\").parquet(storage_path)\n",
    "\n",
    "print(\"âœ… Sample data saved successfully\")\n",
    "print(f\"\\nðŸ“ File location: {storage_path}\")\n",
    "print(f\"   Format: Parquet\")\n",
    "print(f\"   Rows: {sales_df.count()}\")\n",
    "print(f\"   Columns: {len(sales_df.columns)}\")\n",
    "\n",
    "# Verify file exists\n",
    "try:\n",
    "    files = dbutils.fs.ls(\"/tmp/analyst_agent_demo/\")\n",
    "    print(f\"\\nâœ… Verification: Found {len(files)} file(s) in directory\")\n",
    "    for f in files:\n",
    "        print(f\"   â€¢ {f.name} ({f.size} bytes)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸  Could not verify: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… SAMPLE DATA READY FOR TESTING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49080b47-83ba-4fd6-ae56-cd6b5269cdd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸŽ¯ Demo: Data Analyst Assistant Agent\n",
    "\n",
    "This section demonstrates the complete workflow of the Data Analyst Assistant Agent.\n",
    "\n",
    "## What You'll See:\n",
    "1. **Agent Initialization** - Set up the agent with storage paths\n",
    "2. **File Discovery** - Automatically find and catalog data files\n",
    "3. **Natural Language Queries** - Ask questions in plain English\n",
    "4. **Query Explanation** - See exactly what the SQL does\n",
    "5. **Result Validation** - Get confidence scores on accuracy\n",
    "6. **Formatted Results** - View clean, readable output\n",
    "\n",
    "---\n",
    "\n",
    "## Demo Scenarios:\n",
    "* ðŸ“Š **Simple Aggregation** - \"Show me total revenue\"\n",
    "* ðŸ—ºï¸ **Dimensional Analysis** - \"Show me revenue by region\"\n",
    "* ðŸ“… **Time-Based Query** - \"Show me sales from last quarter\"\n",
    "* ðŸ” **Filtered Query** - \"Show me sales in the West region\"\n",
    "* ðŸ“Š **KPI Calculation** - \"What's the average order value?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bb9e0a3-8a6c-4602-bbe9-aee28974405d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo 1: Initialize Agent and Discover Files"
    }
   },
   "outputs": [],
   "source": [
    "# Demo 1: Initialize the agent and discover data files\n",
    "\n",
    "print(\"ðŸš€ DEMO 1: Agent Initialization & File Discovery\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Step 1: Initialize the agent\n",
    "print(\"Step 1: Initializing Data Analyst Agent...\\n\")\n",
    "agent = DataAnalystAgent(spark, storage_paths=[\"/tmp/analyst_agent_demo/\"])\n",
    "\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Step 2: Discover files\n",
    "print(\"Step 2: Discovering data files...\\n\")\n",
    "discovered_files = agent.discover_files()\n",
    "\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Step 3: Show catalog summary\n",
    "print(\"Step 3: Data Catalog Summary\\n\")\n",
    "catalog = agent.get_catalog_summary()\n",
    "print(f\"ðŸ“Š Total data sources: {catalog['total_sources']}\")\n",
    "print(f\"ðŸ“ File formats: {', '.join(catalog['formats'])}\")\n",
    "print(f\"ðŸ“Š Total columns: {catalog['total_columns']}\")\n",
    "print(f\"\\nðŸ’¾ Available sources:\")\n",
    "for source in catalog['sources']:\n",
    "    print(f\"   â€¢ {source}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Agent is ready to answer requests!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9245365-01db-46e9-ace2-8cac061e6a76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo 2: Simple Aggregation Query"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f24a47c-8e86-488c-9d79-298bc9fcdd3e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo 3: Dimensional Analysis"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a541b6f-182b-4eb9-8e50-b0175428171e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo 4: Category Analysis"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4e8121e-41c7-45b9-92cd-8d2de4e490cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo 5: Time-Based Query"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "463fc5b3-e0a6-4043-9113-cb118052ff4b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo 6: View Query History"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8496f44a-bc92-4e03-9bcb-ccac0dded543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Analyst Assistant Agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
