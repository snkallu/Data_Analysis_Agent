{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d54d2c77-337e-4cb9-856f-974ffb69370a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f68602b-7653-4da6-903b-812baa285d1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports and Data Classes"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16d71f2e-870b-4270-9be0-1a6506aebebf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Connect to ADLS Container"
    }
   },
   "outputs": [],
   "source": [
    "# ADLS Gen2 Configuration\n",
    "storage_account_name = \"prepdatalakegen2\"\n",
    "container_name = \"bestsellingbooks\"\n",
    "folder_path = \"Bronze\"\n",
    "\n",
    "# SAS Token (without the leading '?')\n",
    "sas_token = \"sv=2024-11-04&ss=bfqt&srt=co&sp=rwdlacupyx&se=2026-03-12T01:33:01Z&st=2026-02-11T18:18:01Z&spr=https&sig=%2F5bviHwjQMniZatjqkZV%2BRDGisHOu3Bd0PscZ7c8LHw%3D\"\n",
    "\n",
    "# Construct the ADLS path with SAS token embedded\n",
    "# This approach works on serverless compute without spark.conf.set\n",
    "adls_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{folder_path}?{sas_token}\"\n",
    "\n",
    "print(f\"ADLS Path configured: abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{folder_path}/\")\n",
    "print(\"‚úÖ Authentication configured successfully!\")\n",
    "print(\"\\nNote: SAS token is embedded in the path for serverless compute compatibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b99bdec-d8d7-4f34-8507-f5bf9f355462",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Data from ADLS"
    }
   },
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è IMPORTANT: Serverless Compute Limitation\n",
    "# Serverless compute requires Unity Catalog external locations to access external ADLS storage.\n",
    "# Direct SAS token authentication is NOT supported on serverless compute.\n",
    "\n",
    "print(\"‚ùå Serverless Compute Limitation Detected\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚ö†Ô∏è  Serverless compute cannot use SAS tokens directly.\")\n",
    "print(\"\\nYou have 2 options:\\n\")\n",
    "\n",
    "print(\"OPTION 1: Create Unity Catalog External Location (Recommended)\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Ask your workspace admin to:\")\n",
    "print(\"  1. Create a storage credential with Azure service principal or managed identity\")\n",
    "print(\"  2. Create an external location for:\")\n",
    "print(f\"     abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{folder_path}/\")\n",
    "print(\"  3. Grant you READ access to the external location\")\n",
    "print(\"\\nOnce setup, you can access data directly without SAS tokens.\\n\")\n",
    "\n",
    "print(\"OPTION 2: Switch to Classic Cluster\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  1. Create a classic all-purpose cluster (not serverless)\")\n",
    "print(\"  2. Attach this notebook to that cluster\")\n",
    "print(\"  3. The SAS token configuration will work on classic clusters\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚ÑπÔ∏è  Documentation: https://learn.microsoft.com/en-us/azure/databricks/compute/serverless/limitations/\")\n",
    "print(\"\\nWould you like me to:\")\n",
    "print(\"  A) Provide code for Unity Catalog external location access\")\n",
    "print(\"  B) Provide code optimized for classic cluster with SAS token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40fb7f87-17d5-4b62-bca5-69cbaf54891e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Alternative: Generate Sample Data"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dc6d54d-2cee-4e73-ab90-a89546398896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41c386a2-660f-42b7-83a8-b424401b1226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0b25420-f918-4d51-a7ec-895733463b47",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup: Import Agent Module"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71f36834-4902-4abb-914a-ea200260f754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Analyst Assistant Agent\n",
    "\n",
    "## üéØ Project Goal\n",
    "Build an intelligent agent that supports data analysts by:\n",
    "* **Answering data requests** with 100% accurate results\n",
    "* **Querying source files** directly from storage containers\n",
    "* **Breaking down queries** into clear, understandable steps\n",
    "* **Generating KPIs and metrics** for management reports\n",
    "* **Validating results** to ensure accuracy and reliability\n",
    "\n",
    "---\n",
    "\n",
    "## üë• Target Users\n",
    "* **Data Analysts** - Daily data queries and analysis\n",
    "* **Upper Management** - KPIs, metrics, and executive reports\n",
    "* **Data Engineers** - Supporting infrastructure and data quality\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Architecture Components\n",
    "\n",
    "### 1. **File Discovery Engine**\n",
    "   * Scans storage containers (Volumes, DBFS, cloud storage)\n",
    "   * Detects file formats (CSV, Parquet, JSON, Delta)\n",
    "   * Builds searchable data catalog\n",
    "   * Infers schemas automatically\n",
    "\n",
    "### 2. **Natural Language Query Parser**\n",
    "   * Understands analyst requests in plain English\n",
    "   * Extracts intent (metrics, filters, time ranges)\n",
    "   * Maps to available data sources\n",
    "   * Handles ambiguity with clarifying questions\n",
    "\n",
    "### 3. **SQL Generation Engine**\n",
    "   * Converts requests to optimized SQL\n",
    "   * Supports complex aggregations and joins\n",
    "   * Applies best practices (predicate pushdown, column pruning)\n",
    "   * Generates efficient queries for large datasets\n",
    "\n",
    "### 4. **Query Explanation System**\n",
    "   * Breaks down SQL into plain English steps\n",
    "   * Shows data sources and transformations\n",
    "   * Visualizes query logic flow\n",
    "   * Explains calculations and business logic\n",
    "\n",
    "### 5. **Execution & Validation Layer**\n",
    "   * Executes queries with error handling\n",
    "   * Validates results (null checks, range validation)\n",
    "   * Detects anomalies and outliers\n",
    "   * Provides confidence scores\n",
    "\n",
    "### 6. **Result Formatting & Reporting**\n",
    "   * Formats output for readability\n",
    "   * Generates visualizations\n",
    "   * Creates exportable reports\n",
    "   * Supports multiple output formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b013cb4-c758-4d4d-999f-0305590d5aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîÑ Agent Workflow\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    DATA ANALYST REQUEST                         ‚îÇ\n",
    "‚îÇ  \"Show me total revenue by region for last quarter\"             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚îÇ\n",
    "                         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  STEP 1: UNDERSTAND REQUEST                                     ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Parse natural language                                       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Extract: metric=revenue, dimension=region, time=last_quarter ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Identify required data sources                               ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚îÇ\n",
    "                         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  STEP 2: DISCOVER DATA SOURCES                                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Scan storage containers                                      ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Find relevant files: sales_data.parquet, regions.csv         ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Load schemas and metadata                                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚îÇ\n",
    "                         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  STEP 3: GENERATE SQL QUERY                                     ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Build optimized SQL                                          ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Apply filters (date range)                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Add aggregations (SUM, GROUP BY)                             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Optimize for performance                                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚îÇ\n",
    "                         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  STEP 4: EXPLAIN QUERY (Before Execution)                       ‚îÇ\n",
    "‚îÇ  ‚úì Data Source: /volumes/sales/data/sales_2024.parquet         ‚îÇ\n",
    "‚îÇ  ‚úì Filter: date >= '2024-10-01' AND date <= '2024-12-31'       ‚îÇ\n",
    "‚îÇ  ‚úì Calculation: SUM(amount) AS total_revenue                    ‚îÇ\n",
    "‚îÇ  ‚úì Grouping: BY region                                          ‚îÇ\n",
    "‚îÇ  ‚úì Expected rows: ~5 regions                                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚îÇ\n",
    "                         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  STEP 5: EXECUTE QUERY                                          ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Run SQL against data files                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Monitor execution time                                       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Capture any errors                                           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚îÇ\n",
    "                         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  STEP 6: VALIDATE RESULTS                                       ‚îÇ\n",
    "‚îÇ  ‚úì Row count check: 5 rows (expected ~5) ‚úì                     ‚îÇ\n",
    "‚îÇ  ‚úì Null check: No nulls in revenue ‚úì                           ‚îÇ\n",
    "‚îÇ  ‚úì Range check: Revenue values reasonable ‚úì                    ‚îÇ\n",
    "‚îÇ  ‚úì Confidence score: 98%                                        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚îÇ\n",
    "                         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  STEP 7: FORMAT & PRESENT RESULTS                               ‚îÇ\n",
    "‚îÇ  üìä Total Revenue by Region (Q4 2024)                           ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ\n",
    "‚îÇ  ‚îÇ Region       ‚îÇ Total Revenue   ‚îÇ                            ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                            ‚îÇ\n",
    "‚îÇ  ‚îÇ North        ‚îÇ $1,234,567      ‚îÇ                            ‚îÇ\n",
    "‚îÇ  ‚îÇ South        ‚îÇ $987,654        ‚îÇ                            ‚îÇ\n",
    "‚îÇ  ‚îÇ East         ‚îÇ $1,456,789      ‚îÇ                            ‚îÇ\n",
    "‚îÇ  ‚îÇ West         ‚îÇ $1,098,765      ‚îÇ                            ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ\n",
    "‚îÇ  ‚úÖ Results validated with 98% confidence                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ff5ad0-cfff-453b-8581-4a0058da1f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ú® Key Features\n",
    "\n",
    "### üéØ Accuracy Guarantees\n",
    "* **Schema validation** - Verify data types and structure\n",
    "* **Result validation** - Check for nulls, outliers, anomalies\n",
    "* **Confidence scoring** - Rate result reliability (0-100%)\n",
    "* **Audit trail** - Log all queries and results\n",
    "* **Data lineage** - Track data source to result\n",
    "\n",
    "### üìù Query Explanation\n",
    "* **Plain English breakdown** - No SQL jargon\n",
    "* **Step-by-step logic** - Show transformation flow\n",
    "* **Data source transparency** - Which files are used\n",
    "* **Calculation details** - How metrics are computed\n",
    "* **Visual query plans** - Diagram query execution\n",
    "\n",
    "### üóÇÔ∏è Data Source Management\n",
    "* **Auto-discovery** - Find files in storage containers\n",
    "* **Format detection** - CSV, Parquet, JSON, Delta, Avro\n",
    "* **Schema inference** - Automatic column type detection\n",
    "* **Metadata catalog** - Searchable data inventory\n",
    "* **Version tracking** - Handle schema evolution\n",
    "\n",
    "### üìä KPI & Metrics Library\n",
    "* **Pre-built metrics** - Revenue, growth, conversion, churn\n",
    "* **Custom definitions** - Define new KPIs\n",
    "* **Time intelligence** - YoY, MoM, QoQ comparisons\n",
    "* **Dimensional analysis** - Slice by region, product, customer\n",
    "* **Trend analysis** - Historical patterns\n",
    "\n",
    "### üöÄ Performance Optimization\n",
    "* **Query optimization** - Predicate pushdown, column pruning\n",
    "* **Caching** - Reuse results for similar queries\n",
    "* **Incremental processing** - Only query new data\n",
    "* **Parallel execution** - Leverage Spark parallelism\n",
    "* **Resource management** - Efficient memory usage\n",
    "\n",
    "### üîí Security & Governance\n",
    "* **Access control** - Respect Unity Catalog permissions\n",
    "* **Data masking** - Hide sensitive fields\n",
    "* **Audit logging** - Track all data access\n",
    "* **Compliance** - GDPR, HIPAA support\n",
    "* **Data quality** - Flag issues and anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe8222d2-0cce-4014-98ad-1d13fee10f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üíº Example Use Cases\n",
    "\n",
    "### Use Case 1: Daily Sales Report\n",
    "**Analyst Request:**\n",
    "> \"Show me yesterday's sales by product category\"\n",
    "\n",
    "**Agent Actions:**\n",
    "1. Identifies date filter: yesterday\n",
    "2. Finds sales data files\n",
    "3. Generates SQL with date filter and GROUP BY\n",
    "4. Explains: \"Querying sales_2024.parquet, filtering for 2024-02-09, summing sales by category\"\n",
    "5. Validates: 12 categories found, no nulls\n",
    "6. Returns formatted table with totals\n",
    "\n",
    "---\n",
    "\n",
    "### Use Case 2: Executive KPI Dashboard\n",
    "**Management Request:**\n",
    "> \"What's our month-over-month revenue growth?\"\n",
    "\n",
    "**Agent Actions:**\n",
    "1. Recognizes KPI: MoM revenue growth\n",
    "2. Calculates current month and previous month revenue\n",
    "3. Computes growth percentage\n",
    "4. Explains: \"Comparing Jan 2026 ($X) vs Dec 2025 ($Y), growth = Z%\"\n",
    "5. Validates: Both months have complete data\n",
    "6. Returns growth metric with confidence score\n",
    "\n",
    "---\n",
    "\n",
    "### Use Case 3: Ad-hoc Analysis\n",
    "**Analyst Request:**\n",
    "> \"Which customers in the West region spent more than $10,000 last quarter?\"\n",
    "\n",
    "**Agent Actions:**\n",
    "1. Parses filters: region=West, amount>10000, time=last_quarter\n",
    "2. Finds customer and transaction files\n",
    "3. Generates JOIN query\n",
    "4. Explains: \"Joining customers.csv with transactions.parquet, filtering by region and amount\"\n",
    "5. Validates: 47 customers found, all amounts > $10,000\n",
    "6. Returns customer list with spend amounts\n",
    "\n",
    "---\n",
    "\n",
    "### Use Case 4: Data Quality Check\n",
    "**Analyst Request:**\n",
    "> \"Are there any missing values in our customer email addresses?\"\n",
    "\n",
    "**Agent Actions:**\n",
    "1. Identifies data quality query\n",
    "2. Finds customer data file\n",
    "3. Generates null check query\n",
    "4. Explains: \"Counting NULL and empty strings in email column\"\n",
    "5. Validates: Found 23 missing emails out of 10,000 records\n",
    "6. Returns count and percentage, flags data quality issue\n",
    "\n",
    "---\n",
    "\n",
    "### Use Case 5: Trend Analysis\n",
    "**Management Request:**\n",
    "> \"Show me weekly active users for the past 6 months\"\n",
    "\n",
    "**Agent Actions:**\n",
    "1. Recognizes time-series analysis\n",
    "2. Finds user activity logs\n",
    "3. Generates weekly aggregation query\n",
    "4. Explains: \"Counting distinct users per week from Aug 2025 to Jan 2026\"\n",
    "5. Validates: 26 weeks of data, no gaps\n",
    "6. Returns time-series data with trend visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12a2709-f04e-4a48-a0e4-4c1c804a95ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Agent Core - Imports and Data Classes"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, count, sum as spark_sum, avg, min as spark_min, max as spark_max\n",
    "from pyspark.sql.functions import current_timestamp, lit, when, trim, lower, upper, to_date, datediff\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class DataSource:\n",
    "    \"\"\"Represents a discovered data source.\"\"\"\n",
    "    path: str\n",
    "    format: str\n",
    "    schema: StructType\n",
    "    row_count: Optional[int] = None\n",
    "    size_bytes: Optional[int] = None\n",
    "    last_modified: Optional[str] = None\n",
    "    columns: List[str] = field(default_factory=list)\n",
    "    \n",
    "@dataclass\n",
    "class QueryRequest:\n",
    "    \"\"\"Represents an analyst's data request.\"\"\"\n",
    "    original_text: str\n",
    "    intent: str\n",
    "    metrics: List[str] = field(default_factory=list)\n",
    "    dimensions: List[str] = field(default_factory=list)\n",
    "    filters: Dict[str, Any] = field(default_factory=dict)\n",
    "    time_range: Optional[Dict[str, str]] = None\n",
    "    \n",
    "@dataclass\n",
    "class QueryResult:\n",
    "    \"\"\"Represents query execution results with metadata.\"\"\"\n",
    "    data: DataFrame\n",
    "    sql_query: str\n",
    "    explanation: List[str]\n",
    "    execution_time_ms: float\n",
    "    row_count: int\n",
    "    validation_score: float\n",
    "    warnings: List[str] = field(default_factory=list)\n",
    "    data_sources: List[str] = field(default_factory=list)\n",
    "\n",
    "print(\"‚úÖ Data classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754b78c9-7096-460f-8016-93bda2f2b6a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DataAnalystAgent Class - Initialization"
    }
   },
   "outputs": [],
   "source": [
    "class DataAnalystAgent:\n",
    "    \"\"\"\n",
    "    AI Agent for supporting data analysts with accurate, explainable query results.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spark: SparkSession, storage_paths: List[str] = None):\n",
    "        self.spark = spark\n",
    "        self.storage_paths = storage_paths or []\n",
    "        self.data_catalog: Dict[str, DataSource] = {}\n",
    "        self.query_history: List[QueryResult] = []\n",
    "        self.kpi_definitions: Dict[str, Dict] = {}\n",
    "        \n",
    "        # Initialize KPI library\n",
    "        self._initialize_kpi_library()\n",
    "        \n",
    "        print(\"ü§ñ Data Analyst Agent initialized\")\n",
    "        print(f\"   Spark version: {spark.version}\")\n",
    "        print(f\"   Storage paths: {len(self.storage_paths)} configured\")\n",
    "    \n",
    "    def _initialize_kpi_library(self):\n",
    "        \"\"\"Initialize common KPI definitions.\"\"\"\n",
    "        self.kpi_definitions = {\n",
    "            \"revenue\": {\n",
    "                \"metric\": \"SUM(amount)\",\n",
    "                \"description\": \"Total revenue\",\n",
    "                \"required_columns\": [\"amount\"]\n",
    "            },\n",
    "            \"average_order_value\": {\n",
    "                \"metric\": \"AVG(amount)\",\n",
    "                \"description\": \"Average order value\",\n",
    "                \"required_columns\": [\"amount\"]\n",
    "            },\n",
    "            \"customer_count\": {\n",
    "                \"metric\": \"COUNT(DISTINCT customer_id)\",\n",
    "                \"description\": \"Unique customer count\",\n",
    "                \"required_columns\": [\"customer_id\"]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def add_storage_path(self, path: str):\n",
    "        \"\"\"Add a storage path to scan for data files.\"\"\"\n",
    "        if path not in self.storage_paths:\n",
    "            self.storage_paths.append(path)\n",
    "            print(f\"‚úÖ Added storage path: {path}\")\n",
    "    \n",
    "    def get_catalog_summary(self) -> Dict:\n",
    "        \"\"\"Get summary of discovered data sources.\"\"\"\n",
    "        return {\n",
    "            \"total_sources\": len(self.data_catalog),\n",
    "            \"formats\": list(set(ds.format for ds in self.data_catalog.values())),\n",
    "            \"total_columns\": sum(len(ds.columns) for ds in self.data_catalog.values()),\n",
    "            \"sources\": list(self.data_catalog.keys())\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ DataAnalystAgent class initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3bfc140-cc5a-4155-8cb3-7ec6c7238e37",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "File Discovery Methods"
    }
   },
   "outputs": [],
   "source": [
    "def discover_files(self, path: str = None, pattern: str = \"*\") -> List[DataSource]:\n",
    "    \"\"\"\n",
    "    Discover data files in storage containers.\n",
    "    \n",
    "    Args:\n",
    "        path: Specific path to scan (uses configured paths if None)\n",
    "        pattern: File pattern to match (e.g., '*.parquet', 'sales_*')\n",
    "        \n",
    "    Returns:\n",
    "        List of discovered DataSource objects\n",
    "    \"\"\"\n",
    "    print(f\"üîç Discovering data files...\")\n",
    "    \n",
    "    paths_to_scan = [path] if path else self.storage_paths\n",
    "    discovered = []\n",
    "    \n",
    "    for scan_path in paths_to_scan:\n",
    "        try:\n",
    "            # List files using dbutils\n",
    "            try:\n",
    "                files = dbutils.fs.ls(scan_path)\n",
    "            except:\n",
    "                print(f\"   ‚ö†Ô∏è  Could not list directory: {scan_path}\")\n",
    "                continue\n",
    "            \n",
    "            for file_info in files:\n",
    "                file_path = file_info.path\n",
    "                file_name = file_info.name\n",
    "                \n",
    "                # Skip directories and hidden files\n",
    "                if file_info.isDir() or file_name.startswith('.'):\n",
    "                    continue\n",
    "                \n",
    "                # Detect file format\n",
    "                file_format = self._detect_format(file_name)\n",
    "                if not file_format:\n",
    "                    continue\n",
    "                \n",
    "                # Try to read schema\n",
    "                try:\n",
    "                    if file_format == \"delta\":\n",
    "                        df = self.spark.read.format(\"delta\").load(file_path)\n",
    "                    elif file_format == \"parquet\":\n",
    "                        df = self.spark.read.parquet(file_path)\n",
    "                    elif file_format == \"csv\":\n",
    "                        df = self.spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "                    elif file_format == \"json\":\n",
    "                        df = self.spark.read.json(file_path)\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    # Create DataSource object\n",
    "                    data_source = DataSource(\n",
    "                        path=file_path,\n",
    "                        format=file_format,\n",
    "                        schema=df.schema,\n",
    "                        columns=[field.name for field in df.schema.fields],\n",
    "                        size_bytes=file_info.size,\n",
    "                        last_modified=str(file_info.modificationTime)\n",
    "                    )\n",
    "                    \n",
    "                    # Add to catalog\n",
    "                    self.data_catalog[file_name] = data_source\n",
    "                    discovered.append(data_source)\n",
    "                    \n",
    "                    print(f\"   ‚úÖ Found: {file_name} ({file_format}, {len(data_source.columns)} columns)\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error reading {file_name}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error scanning {scan_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Discovery complete: {len(discovered)} files found\")\n",
    "    return discovered\n",
    "\n",
    "def _detect_format(self, filename: str) -> Optional[str]:\n",
    "    \"\"\"Detect file format from filename.\"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    if filename_lower.endswith('.parquet'):\n",
    "        return 'parquet'\n",
    "    elif filename_lower.endswith('.csv'):\n",
    "        return 'csv'\n",
    "    elif filename_lower.endswith('.json'):\n",
    "        return 'json'\n",
    "    elif filename_lower.endswith('.delta') or '_delta_log' in filename_lower:\n",
    "        return 'delta'\n",
    "    return None\n",
    "\n",
    "# Add methods to class\n",
    "DataAnalystAgent.discover_files = discover_files\n",
    "DataAnalystAgent._detect_format = _detect_format\n",
    "\n",
    "print(\"‚úÖ File discovery methods added to DataAnalystAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a5d3396-0ca3-4cb2-9606-02662e3e7dbc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Request Understanding Methods"
    }
   },
   "outputs": [],
   "source": [
    "def understand_request(self, request_text: str) -> QueryRequest:\n",
    "    \"\"\"\n",
    "    Parse natural language request into structured QueryRequest.\n",
    "    \n",
    "    Args:\n",
    "        request_text: Analyst's request in natural language\n",
    "        \n",
    "    Returns:\n",
    "        QueryRequest object with parsed intent and parameters\n",
    "    \"\"\"\n",
    "    print(f\"üß† Understanding request: '{request_text}'\")\n",
    "    \n",
    "    request_lower = request_text.lower()\n",
    "    \n",
    "    # Detect intent\n",
    "    intent = self._detect_intent(request_lower)\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics = self._extract_metrics(request_lower)\n",
    "    \n",
    "    # Extract dimensions\n",
    "    dimensions = self._extract_dimensions(request_lower)\n",
    "    \n",
    "    # Extract filters\n",
    "    filters = self._extract_filters(request_lower)\n",
    "    \n",
    "    # Extract time range\n",
    "    time_range = self._extract_time_range(request_lower)\n",
    "    \n",
    "    query_request = QueryRequest(\n",
    "        original_text=request_text,\n",
    "        intent=intent,\n",
    "        metrics=metrics,\n",
    "        dimensions=dimensions,\n",
    "        filters=filters,\n",
    "        time_range=time_range\n",
    "    )\n",
    "    \n",
    "    print(f\"   Intent: {intent}\")\n",
    "    print(f\"   Metrics: {metrics}\")\n",
    "    print(f\"   Dimensions: {dimensions}\")\n",
    "    if filters:\n",
    "        print(f\"   Filters: {filters}\")\n",
    "    if time_range:\n",
    "        print(f\"   Time range: {time_range}\")\n",
    "    \n",
    "    return query_request\n",
    "\n",
    "def _detect_intent(self, text: str) -> str:\n",
    "    \"\"\"Detect query intent from text.\"\"\"\n",
    "    if any(word in text for word in ['total', 'sum', 'count', 'average', 'avg']):\n",
    "        return 'aggregate'\n",
    "    elif any(word in text for word in ['growth', 'change', 'trend', 'over time']):\n",
    "        return 'trend'\n",
    "    elif any(word in text for word in ['kpi', 'metric', 'performance']):\n",
    "        return 'kpi'\n",
    "    elif any(word in text for word in ['where', 'filter', 'only', 'specific']):\n",
    "        return 'filter'\n",
    "    elif any(word in text for word in ['join', 'combine', 'merge', 'with']):\n",
    "        return 'join'\n",
    "    else:\n",
    "        return 'general'\n",
    "\n",
    "def _extract_metrics(self, text: str) -> List[str]:\n",
    "    \"\"\"Extract metrics from text.\"\"\"\n",
    "    metrics = []\n",
    "    metric_keywords = {\n",
    "        'revenue': ['revenue', 'sales', 'income'],\n",
    "        'count': ['count', 'number of', 'how many'],\n",
    "        'average': ['average', 'avg', 'mean'],\n",
    "        'total': ['total', 'sum'],\n",
    "        'max': ['maximum', 'max', 'highest'],\n",
    "        'min': ['minimum', 'min', 'lowest']\n",
    "    }\n",
    "    \n",
    "    for metric, keywords in metric_keywords.items():\n",
    "        if any(kw in text for kw in keywords):\n",
    "            metrics.append(metric)\n",
    "    \n",
    "    return metrics if metrics else ['count']\n",
    "\n",
    "def _extract_dimensions(self, text: str) -> List[str]:\n",
    "    \"\"\"Extract dimensions (group by columns) from text.\"\"\"\n",
    "    dimensions = []\n",
    "    \n",
    "    # Common dimensions\n",
    "    common_dims = ['region', 'category', 'product', 'customer', 'date', 'month', 'year', 'quarter']\n",
    "    \n",
    "    for dim in common_dims:\n",
    "        if dim in text:\n",
    "            dimensions.append(dim)\n",
    "    \n",
    "    return dimensions\n",
    "\n",
    "def _extract_filters(self, text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract filter conditions from text.\"\"\"\n",
    "    filters = {}\n",
    "    \n",
    "    # Extract comparison filters\n",
    "    amount_pattern = r'(more than|greater than|less than|at least|over|under)\\s+(\\$?[\\d,]+)'\n",
    "    matches = re.findall(amount_pattern, text)\n",
    "    if matches:\n",
    "        operator, value = matches[0]\n",
    "        value_clean = value.replace('$', '').replace(',', '')\n",
    "        if 'more than' in operator or 'greater than' in operator or 'over' in operator:\n",
    "            filters['amount'] = {'operator': '>', 'value': float(value_clean)}\n",
    "        elif 'less than' in operator or 'under' in operator:\n",
    "            filters['amount'] = {'operator': '<', 'value': float(value_clean)}\n",
    "    \n",
    "    # Extract region/category filters\n",
    "    if 'in the' in text or 'from' in text:\n",
    "        for word in text.split():\n",
    "            if word.capitalize() in ['North', 'South', 'East', 'West']:\n",
    "                filters['region'] = word.capitalize()\n",
    "    \n",
    "    return filters\n",
    "\n",
    "def _extract_time_range(self, text: str) -> Optional[Dict[str, str]]:\n",
    "    \"\"\"Extract time range from text.\"\"\"\n",
    "    today = datetime.now()\n",
    "    \n",
    "    if 'yesterday' in text:\n",
    "        date = today - timedelta(days=1)\n",
    "        return {'start': date.strftime('%Y-%m-%d'), 'end': date.strftime('%Y-%m-%d')}\n",
    "    elif 'last week' in text:\n",
    "        start = today - timedelta(days=7)\n",
    "        return {'start': start.strftime('%Y-%m-%d'), 'end': today.strftime('%Y-%m-%d')}\n",
    "    elif 'last month' in text:\n",
    "        start = today - timedelta(days=30)\n",
    "        return {'start': start.strftime('%Y-%m-%d'), 'end': today.strftime('%Y-%m-%d')}\n",
    "    elif 'last quarter' in text or 'q4' in text or 'quarter' in text:\n",
    "        start = today - timedelta(days=90)\n",
    "        return {'start': start.strftime('%Y-%m-%d'), 'end': today.strftime('%Y-%m-%d')}\n",
    "    elif 'last year' in text:\n",
    "        start = today - timedelta(days=365)\n",
    "        return {'start': start.strftime('%Y-%m-%d'), 'end': today.strftime('%Y-%m-%d')}\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Add methods to class\n",
    "DataAnalystAgent.understand_request = understand_request\n",
    "DataAnalystAgent._detect_intent = _detect_intent\n",
    "DataAnalystAgent._extract_metrics = _extract_metrics\n",
    "DataAnalystAgent._extract_dimensions = _extract_dimensions\n",
    "DataAnalystAgent._extract_filters = _extract_filters\n",
    "DataAnalystAgent._extract_time_range = _extract_time_range\n",
    "\n",
    "print(\"‚úÖ Request understanding methods added to DataAnalystAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9128337-7a17-4017-98a8-b62d1652de75",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query Generation Methods"
    }
   },
   "outputs": [],
   "source": [
    "def generate_query(self, request: QueryRequest, data_source: DataSource) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Generate SQL query from QueryRequest.\n",
    "    \n",
    "    Args:\n",
    "        request: Parsed query request\n",
    "        data_source: Data source to query\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (SQL query string, explanation steps)\n",
    "    \"\"\"\n",
    "    print(f\"üõ†Ô∏è Generating SQL query...\")\n",
    "    \n",
    "    explanation = []\n",
    "    explanation.append(f\"Data source: {data_source.path} ({data_source.format} format)\")\n",
    "    \n",
    "    # Build SELECT clause\n",
    "    select_parts = []\n",
    "    \n",
    "    # Add dimensions\n",
    "    for dim in request.dimensions:\n",
    "        if dim in data_source.columns:\n",
    "            select_parts.append(dim)\n",
    "            explanation.append(f\"Group results by: {dim}\")\n",
    "    \n",
    "    # Add metrics\n",
    "    for metric in request.metrics:\n",
    "        if metric == 'revenue' or metric == 'total':\n",
    "            if 'amount' in data_source.columns:\n",
    "                select_parts.append(\"SUM(amount) AS total_amount\")\n",
    "                explanation.append(\"Calculate: Total sum of amount column\")\n",
    "            elif 'revenue' in data_source.columns:\n",
    "                select_parts.append(\"SUM(revenue) AS total_revenue\")\n",
    "                explanation.append(\"Calculate: Total sum of revenue column\")\n",
    "        elif metric == 'count':\n",
    "            select_parts.append(\"COUNT(*) AS record_count\")\n",
    "            explanation.append(\"Calculate: Count of all records\")\n",
    "        elif metric == 'average':\n",
    "            if 'amount' in data_source.columns:\n",
    "                select_parts.append(\"AVG(amount) AS average_amount\")\n",
    "                explanation.append(\"Calculate: Average of amount column\")\n",
    "    \n",
    "    # Default to COUNT(*) if no metrics\n",
    "    if not any('AS' in part for part in select_parts):\n",
    "        select_parts.append(\"COUNT(*) AS record_count\")\n",
    "        explanation.append(\"Calculate: Count of all records\")\n",
    "    \n",
    "    select_clause = \"SELECT \" + \", \".join(select_parts)\n",
    "    \n",
    "    # Build FROM clause using read_files()\n",
    "    from_clause = f\"FROM read_files('{data_source.path}')\"\n",
    "    \n",
    "    # Build WHERE clause\n",
    "    where_conditions = []\n",
    "    \n",
    "    # Add time range filter\n",
    "    if request.time_range:\n",
    "        date_col = next((col for col in data_source.columns if 'date' in col.lower()), None)\n",
    "        if date_col:\n",
    "            where_conditions.append(f\"{date_col} >= '{request.time_range['start']}'\")\n",
    "            where_conditions.append(f\"{date_col} <= '{request.time_range['end']}'\")\n",
    "            explanation.append(f\"Filter: Date range from {request.time_range['start']} to {request.time_range['end']}\")\n",
    "    \n",
    "    # Add other filters\n",
    "    for col_name, filter_spec in request.filters.items():\n",
    "        if col_name in data_source.columns:\n",
    "            if isinstance(filter_spec, dict):\n",
    "                operator = filter_spec.get('operator', '=')\n",
    "                value = filter_spec.get('value')\n",
    "                where_conditions.append(f\"{col_name} {operator} {value}\")\n",
    "                explanation.append(f\"Filter: {col_name} {operator} {value}\")\n",
    "            else:\n",
    "                where_conditions.append(f\"{col_name} = '{filter_spec}'\")\n",
    "                explanation.append(f\"Filter: {col_name} = {filter_spec}\")\n",
    "    \n",
    "    where_clause = \"WHERE \" + \" AND \".join(where_conditions) if where_conditions else \"\"\n",
    "    \n",
    "    # Build GROUP BY clause\n",
    "    group_by_clause = \"\"\n",
    "    if request.dimensions:\n",
    "        group_by_cols = [dim for dim in request.dimensions if dim in data_source.columns]\n",
    "        if group_by_cols:\n",
    "            group_by_clause = \"GROUP BY \" + \", \".join(group_by_cols)\n",
    "    \n",
    "    # Build ORDER BY clause\n",
    "    order_by_clause = \"\"\n",
    "    if request.dimensions:\n",
    "        order_by_clause = f\"ORDER BY {request.dimensions[0]}\"\n",
    "    \n",
    "    # Combine all parts\n",
    "    query_parts = [select_clause, from_clause]\n",
    "    if where_clause:\n",
    "        query_parts.append(where_clause)\n",
    "    if group_by_clause:\n",
    "        query_parts.append(group_by_clause)\n",
    "    if order_by_clause:\n",
    "        query_parts.append(order_by_clause)\n",
    "    \n",
    "    sql_query = \"\\n\".join(query_parts)\n",
    "    \n",
    "    print(f\"   ‚úÖ Query generated ({len(explanation)} explanation steps)\")\n",
    "    return sql_query, explanation\n",
    "\n",
    "# Add method to class\n",
    "DataAnalystAgent.generate_query = generate_query\n",
    "\n",
    "print(\"‚úÖ Query generation methods added to DataAnalystAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18a3d824-75ed-4189-aa3a-78236ad5b3eb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query Execution and Validation Methods"
    }
   },
   "outputs": [],
   "source": [
    "def execute_query(self, sql_query: str, explanation: List[str], data_sources: List[str]) -> QueryResult:\n",
    "    \"\"\"\n",
    "    Execute SQL query and validate results.\n",
    "    \n",
    "    Args:\n",
    "        sql_query: SQL query to execute\n",
    "        explanation: Query explanation steps\n",
    "        data_sources: List of data source paths used\n",
    "        \n",
    "    Returns:\n",
    "        QueryResult with data and validation metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚ñ∂Ô∏è Executing query...\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    warnings = []\n",
    "    \n",
    "    try:\n",
    "        # Execute query\n",
    "        result_df = self.spark.sql(sql_query)\n",
    "        \n",
    "        # Get row count\n",
    "        row_count = result_df.count()\n",
    "        \n",
    "        execution_time = (datetime.now() - start_time).total_seconds() * 1000\n",
    "        \n",
    "        print(f\"   ‚úÖ Query executed in {execution_time:.0f}ms\")\n",
    "        print(f\"   ‚úÖ Returned {row_count} rows\")\n",
    "        \n",
    "        # Validate results\n",
    "        validation_score, validation_warnings = self._validate_results(result_df, row_count)\n",
    "        warnings.extend(validation_warnings)\n",
    "        \n",
    "        return QueryResult(\n",
    "            data=result_df,\n",
    "            sql_query=sql_query,\n",
    "            explanation=explanation,\n",
    "            execution_time_ms=execution_time,\n",
    "            row_count=row_count,\n",
    "            validation_score=validation_score,\n",
    "            warnings=warnings,\n",
    "            data_sources=data_sources\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Query execution failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def _validate_results(self, df: DataFrame, row_count: int) -> Tuple[float, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate query results and return confidence score.\n",
    "    \n",
    "    Args:\n",
    "        df: Result DataFrame\n",
    "        row_count: Number of rows returned\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (validation score 0-100, list of warnings)\n",
    "    \"\"\"\n",
    "    score = 100.0\n",
    "    warnings = []\n",
    "    \n",
    "    # Check 1: Empty results\n",
    "    if row_count == 0:\n",
    "        score -= 50\n",
    "        warnings.append(\"‚ö†Ô∏è  Query returned no results\")\n",
    "        return score, warnings\n",
    "    \n",
    "    # Check 2: Null values in results\n",
    "    for col_name in df.columns:\n",
    "        null_count = df.filter(df[col_name].isNull()).count()\n",
    "        if null_count > 0:\n",
    "            null_pct = (null_count / row_count) * 100\n",
    "            if null_pct > 50:\n",
    "                score -= 20\n",
    "                warnings.append(f\"‚ö†Ô∏è  Column '{col_name}' has {null_pct:.1f}% null values\")\n",
    "            elif null_pct > 10:\n",
    "                score -= 5\n",
    "                warnings.append(f\"‚ö†Ô∏è  Column '{col_name}' has {null_pct:.1f}% null values\")\n",
    "    \n",
    "    # Check 3: Reasonable row count\n",
    "    if row_count > 1000000:\n",
    "        score -= 10\n",
    "        warnings.append(f\"‚ö†Ô∏è  Large result set ({row_count:,} rows) - consider adding filters\")\n",
    "    \n",
    "    # Check 4: Data type consistency for amount columns\n",
    "    for field in df.schema.fields:\n",
    "        if 'amount' in field.name.lower() or 'revenue' in field.name.lower():\n",
    "            # Check for negative values in amount columns\n",
    "            try:\n",
    "                negative_count = df.filter(df[field.name] < 0).count()\n",
    "                if negative_count > 0:\n",
    "                    score -= 10\n",
    "                    warnings.append(f\"‚ö†Ô∏è  Column '{field.name}' has {negative_count} negative values\")\n",
    "            except:\n",
    "                pass  # Skip if comparison not supported\n",
    "    \n",
    "    return max(score, 0), warnings\n",
    "\n",
    "# Add methods to class\n",
    "DataAnalystAgent.execute_query = execute_query\n",
    "DataAnalystAgent._validate_results = _validate_results\n",
    "\n",
    "print(\"‚úÖ Query execution and validation methods added to DataAnalystAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf2de22e-704b-401d-b372-967a70a8ac9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Result Formatting and Explanation Methods"
    }
   },
   "outputs": [],
   "source": [
    "def explain_query(self, result: QueryResult) -> None:\n",
    "    \"\"\"\n",
    "    Print detailed query explanation in plain English.\n",
    "    \n",
    "    Args:\n",
    "        result: QueryResult object\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìù QUERY EXPLANATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüìÑ What this query does:\")\n",
    "    for i, step in enumerate(result.explanation, 1):\n",
    "        print(f\"   {i}. {step}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Data Sources:\")\n",
    "    for source in result.data_sources:\n",
    "        print(f\"   ‚Ä¢ {source}\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Execution Time: {result.execution_time_ms:.0f}ms\")\n",
    "    print(f\"üìà Rows Returned: {result.row_count:,}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Validation Score: {result.validation_score:.0f}%\")\n",
    "    if result.warnings:\n",
    "        print(\"\\n‚ö†Ô∏è  Warnings:\")\n",
    "        for warning in result.warnings:\n",
    "            print(f\"   {warning}\")\n",
    "    else:\n",
    "        print(\"   No issues detected - results are reliable\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä SQL QUERY\")\n",
    "    print(\"=\"*80)\n",
    "    print(result.sql_query)\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def format_results(self, result: QueryResult, limit: int = 100) -> None:\n",
    "    \"\"\"\n",
    "    Display formatted query results.\n",
    "    \n",
    "    Args:\n",
    "        result: QueryResult object\n",
    "        limit: Maximum rows to display\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä QUERY RESULTS ({result.row_count:,} total rows)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if result.row_count == 0:\n",
    "        print(\"‚ö†Ô∏è  No results found\")\n",
    "        return\n",
    "    \n",
    "    # Display results\n",
    "    display(result.data.limit(limit))\n",
    "    \n",
    "    if result.row_count > limit:\n",
    "        print(f\"\\nüìå Showing first {limit} of {result.row_count:,} rows\")\n",
    "\n",
    "# Add methods to class\n",
    "DataAnalystAgent.explain_query = explain_query\n",
    "DataAnalystAgent.format_results = format_results\n",
    "\n",
    "print(\"‚úÖ Result formatting methods added to DataAnalystAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dd3fe2d-ee1f-4403-bef8-3fb0847109c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main Workflow Method - answer_request"
    }
   },
   "outputs": [],
   "source": [
    "def answer_request(self, request_text: str, data_source_name: str = None) -> QueryResult:\n",
    "    \"\"\"\n",
    "    Complete workflow: understand request, generate query, execute, and explain.\n",
    "    \n",
    "    This is the main method analysts will use.\n",
    "    \n",
    "    Args:\n",
    "        request_text: Analyst's request in natural language\n",
    "        data_source_name: Specific data source to use (auto-detect if None)\n",
    "        \n",
    "    Returns:\n",
    "        QueryResult object with data and metadata\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"üöÄ \"*30)\n",
    "    print(\"üöÄ PROCESSING ANALYST REQUEST\")\n",
    "    print(\"üöÄ \"*30 + \"\\n\")\n",
    "    print(f\"üí¨ Request: '{request_text}'\\n\")\n",
    "    \n",
    "    # Step 1: Understand request\n",
    "    request = self.understand_request(request_text)\n",
    "    \n",
    "    # Step 2: Find appropriate data source\n",
    "    if data_source_name:\n",
    "        if data_source_name not in self.data_catalog:\n",
    "            raise ValueError(f\"Data source '{data_source_name}' not found in catalog\")\n",
    "        data_source = self.data_catalog[data_source_name]\n",
    "    else:\n",
    "        # Auto-select first available data source\n",
    "        if not self.data_catalog:\n",
    "            raise ValueError(\"No data sources available. Run discover_files() first.\")\n",
    "        data_source = list(self.data_catalog.values())[0]\n",
    "        print(f\"\\nüíæ Auto-selected data source: {list(self.data_catalog.keys())[0]}\")\n",
    "    \n",
    "    # Step 3: Generate query\n",
    "    sql_query, explanation = self.generate_query(request, data_source)\n",
    "    \n",
    "    # Step 4: Execute query\n",
    "    result = self.execute_query(sql_query, explanation, [data_source.path])\n",
    "    \n",
    "    # Step 5: Explain and display\n",
    "    self.explain_query(result)\n",
    "    self.format_results(result)\n",
    "    \n",
    "    # Save to history\n",
    "    self.query_history.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"‚úÖ \"*30)\n",
    "    print(\"‚úÖ REQUEST COMPLETED SUCCESSFULLY\")\n",
    "    print(\"‚úÖ \"*30 + \"\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Add method to class\n",
    "DataAnalystAgent.answer_request = answer_request\n",
    "\n",
    "print(\"‚úÖ Main workflow method added to DataAnalystAgent\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ DATA ANALYST AGENT MODULE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüéâ The agent is ready to use!\")\n",
    "print(\"\\nQuick Start:\")\n",
    "print(\"  1. agent = DataAnalystAgent(spark, ['/path/to/data'])\")\n",
    "print(\"  2. agent.discover_files()\")\n",
    "print(\"  3. result = agent.answer_request('Show me total revenue by region')\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a488698-6c18-4ffb-a07d-2e20663f897f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Sample Sales Data"
    }
   },
   "outputs": [],
   "source": [
    "# Generate sample sales data for testing\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType, IntegerType\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "print(\"üìä Generating sample sales data...\\n\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Generate dates for the past year\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2025, 2, 10)\n",
    "date_range = [(start_date + timedelta(days=x)).strftime('%Y-%m-%d') \n",
    "              for x in range((end_date - start_date).days)]\n",
    "\n",
    "# Sample data parameters\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "categories = ['Electronics', 'Clothing', 'Food', 'Home', 'Sports']\n",
    "products = {\n",
    "    'Electronics': ['Laptop', 'Phone', 'Tablet', 'Headphones'],\n",
    "    'Clothing': ['Shirt', 'Pants', 'Jacket', 'Shoes'],\n",
    "    'Food': ['Snacks', 'Beverages', 'Frozen', 'Fresh'],\n",
    "    'Home': ['Furniture', 'Decor', 'Kitchen', 'Bedding'],\n",
    "    'Sports': ['Equipment', 'Apparel', 'Footwear', 'Accessories']\n",
    "}\n",
    "\n",
    "# Generate 1000 sales records\n",
    "sales_data = []\n",
    "for i in range(1000):\n",
    "    date = random.choice(date_range)\n",
    "    region = random.choice(regions)\n",
    "    category = random.choice(categories)\n",
    "    product = random.choice(products[category])\n",
    "    amount = round(random.uniform(10, 5000), 2)\n",
    "    quantity = random.randint(1, 10)\n",
    "    customer_id = f\"CUST{random.randint(1000, 9999)}\"\n",
    "    \n",
    "    sales_data.append((\n",
    "        i + 1,\n",
    "        date,\n",
    "        region,\n",
    "        category,\n",
    "        product,\n",
    "        amount,\n",
    "        quantity,\n",
    "        customer_id\n",
    "    ))\n",
    "\n",
    "# Create DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), False),\n",
    "    StructField(\"date\", StringType(), False),\n",
    "    StructField(\"region\", StringType(), False),\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"product\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", StringType(), False)\n",
    "])\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, schema)\n",
    "\n",
    "print(f\"‚úÖ Generated {sales_df.count()} sales records\")\n",
    "print(f\"   Date range: {min(date_range)} to {max(date_range)}\")\n",
    "print(f\"   Regions: {', '.join(regions)}\")\n",
    "print(f\"   Categories: {', '.join(categories)}\")\n",
    "print(\"\\nüìä Sample data preview:\")\n",
    "display(sales_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c731547b-0fd4-4d19-ab54-2548116fedb3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save Sample Data to Storage"
    }
   },
   "outputs": [],
   "source": [
    "# Save sample data to DBFS for testing\n",
    "import os\n",
    "\n",
    "# Define storage path\n",
    "storage_path = \"/tmp/analyst_agent_demo/sales_data.parquet\"\n",
    "\n",
    "print(f\"üíæ Saving sample data to: {storage_path}\\n\")\n",
    "\n",
    "# Save as Parquet\n",
    "sales_df.write.mode(\"overwrite\").parquet(storage_path)\n",
    "\n",
    "print(\"‚úÖ Sample data saved successfully\")\n",
    "print(f\"\\nüìÅ File location: {storage_path}\")\n",
    "print(f\"   Format: Parquet\")\n",
    "print(f\"   Rows: {sales_df.count()}\")\n",
    "print(f\"   Columns: {len(sales_df.columns)}\")\n",
    "\n",
    "# Verify file exists\n",
    "try:\n",
    "    files = dbutils.fs.ls(\"/tmp/analyst_agent_demo/\")\n",
    "    print(f\"\\n‚úÖ Verification: Found {len(files)} file(s) in directory\")\n",
    "    for f in files:\n",
    "        print(f\"   ‚Ä¢ {f.name} ({f.size} bytes)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Could not verify: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ SAMPLE DATA READY FOR TESTING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49080b47-83ba-4fd6-ae56-cd6b5269cdd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéØ Demo: Data Analyst Assistant Agent\n",
    "\n",
    "This section demonstrates the complete workflow of the Data Analyst Assistant Agent.\n",
    "\n",
    "## What You'll See:\n",
    "1. **Agent Initialization** - Set up the agent with storage paths\n",
    "2. **File Discovery** - Automatically find and catalog data files\n",
    "3. **Natural Language Queries** - Ask questions in plain English\n",
    "4. **Query Explanation** - See exactly what the SQL does\n",
    "5. **Result Validation** - Get confidence scores on accuracy\n",
    "6. **Formatted Results** - View clean, readable output\n",
    "\n",
    "---\n",
    "\n",
    "## Demo Scenarios:\n",
    "* üìä **Simple Aggregation** - \"Show me total revenue\"\n",
    "* üó∫Ô∏è **Dimensional Analysis** - \"Show me revenue by region\"\n",
    "* üìÖ **Time-Based Query** - \"Show me sales from last quarter\"\n",
    "* üîç **Filtered Query** - \"Show me sales in the West region\"\n",
    "* üìä **KPI Calculation** - \"What's the average order value?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bb9e0a3-8a6c-4602-bbe9-aee28974405d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo 1: Initialize Agent and Discover Files"
    }
   },
   "outputs": [],
   "source": [
    "# Demo 1: Initialize the agent and discover data files\n",
    "\n",
    "print(\"üöÄ DEMO 1: Agent Initialization & File Discovery\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Step 1: Initialize the agent\n",
    "print(\"Step 1: Initializing Data Analyst Agent...\\n\")\n",
    "agent = DataAnalystAgent(spark, storage_paths=[\"/tmp/analyst_agent_demo/\"])\n",
    "\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Step 2: Discover files\n",
    "print(\"Step 2: Discovering data files...\\n\")\n",
    "discovered_files = agent.discover_files()\n",
    "\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Step 3: Show catalog summary\n",
    "print(\"Step 3: Data Catalog Summary\\n\")\n",
    "catalog = agent.get_catalog_summary()\n",
    "print(f\"üìä Total data sources: {catalog['total_sources']}\")\n",
    "print(f\"üìÅ File formats: {', '.join(catalog['formats'])}\")\n",
    "print(f\"üìä Total columns: {catalog['total_columns']}\")\n",
    "print(f\"\\nüíæ Available sources:\")\n",
    "for source in catalog['sources']:\n",
    "    print(f\"   ‚Ä¢ {source}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Agent is ready to answer requests!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9245365-01db-46e9-ace2-8cac061e6a76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo 2: Simple Aggregation Query"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f24a47c-8e86-488c-9d79-298bc9fcdd3e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo 3: Dimensional Analysis"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a541b6f-182b-4eb9-8e50-b0175428171e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo 4: Category Analysis"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4e8121e-41c7-45b9-92cd-8d2de4e490cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo 5: Time-Based Query"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "463fc5b3-e0a6-4043-9113-cb118052ff4b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Demo 6: View Query History"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8496f44a-bc92-4e03-9bcb-ccac0dded543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4ac5f1-c848-44fa-bd64-1e310a6f593c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Demo 1: Discover & Profile Data Sources\n",
    "# MAGIC \n",
    "# MAGIC This notebook demonstrates the agent's discovery and profiling capabilities.\n",
    "# MAGIC \n",
    "# MAGIC **What this does:**\n",
    "# MAGIC * Discovers all tables in Unity Catalog\n",
    "# MAGIC * Profiles table structure and data quality\n",
    "# MAGIC * Identifies potential issues (nulls, categoricals, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27edea1d-7f5e-44d0-a4c8-e516a699de80",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Agent"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, when, lit, current_timestamp\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "class ETLAgent:\n",
    "    \"\"\"\n",
    "    AI Agent for automated ETL pipeline development and maintenance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spark: SparkSession):\n",
    "        self.spark = spark\n",
    "        self.context = {}\n",
    "        self.pipeline_history = []\n",
    "        \n",
    "    def discover_sources(self, catalog: str = \"prep_databricks\", schema: str = None) -> List[Dict]:\n",
    "        \"\"\"Discover available data sources in Unity Catalog.\"\"\"\n",
    "        print(f\"üîç Discovering sources in {catalog}.{schema or '*'}...\")\n",
    "        \n",
    "        try:\n",
    "            if schema:\n",
    "                tables = self.spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\").collect()\n",
    "            else:\n",
    "                schemas = self.spark.sql(f\"SHOW SCHEMAS IN {catalog}\").collect()\n",
    "                tables = []\n",
    "                for s in schemas:\n",
    "                    schema_name = s.namespace\n",
    "                    try:\n",
    "                        schema_tables = self.spark.sql(f\"SHOW TABLES IN {catalog}.{schema_name}\").collect()\n",
    "                        tables.extend(schema_tables)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            discovered = []\n",
    "            for table in tables:\n",
    "                table_info = {\n",
    "                    \"catalog\": catalog,\n",
    "                    \"schema\": table.database,\n",
    "                    \"name\": table.tableName,\n",
    "                    \"full_name\": f\"{catalog}.{table.database}.{table.tableName}\"\n",
    "                }\n",
    "                discovered.append(table_info)\n",
    "                \n",
    "            print(f\"‚úÖ Found {len(discovered)} tables\")\n",
    "            return discovered\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error discovering sources: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def profile_table(self, table_name: str, sample_size: int = 1000) -> Dict:\n",
    "        \"\"\"Profile a table to understand its structure and data quality.\"\"\"\n",
    "        print(f\"üìä Profiling table: {table_name}\")\n",
    "        \n",
    "        try:\n",
    "            df = self.spark.table(table_name)\n",
    "            total_count = df.count()\n",
    "            \n",
    "            if total_count > sample_size:\n",
    "                df_sample = df.limit(sample_size)\n",
    "            else:\n",
    "                df_sample = df\n",
    "            \n",
    "            profile = {\n",
    "                \"table_name\": table_name,\n",
    "                \"row_count\": total_count,\n",
    "                \"sampled_rows\": df_sample.count(),\n",
    "                \"columns\": [],\n",
    "                \"schema\": df.schema.simpleString()\n",
    "            }\n",
    "            \n",
    "            for field in df.schema.fields:\n",
    "                col_info = {\n",
    "                    \"name\": field.name,\n",
    "                    \"type\": str(field.dataType),\n",
    "                    \"nullable\": field.nullable\n",
    "                }\n",
    "                \n",
    "                null_count = df_sample.filter(df_sample[field.name].isNull()).count()\n",
    "                col_info[\"null_count\"] = null_count\n",
    "                col_info[\"null_percentage\"] = round((null_count / profile[\"sampled_rows\"] * 100), 2) if profile[\"sampled_rows\"] > 0 else 0\n",
    "                \n",
    "                if \"string\" in str(field.dataType).lower():\n",
    "                    empty_count = df_sample.filter((df_sample[field.name] == \"\") | (df_sample[field.name].isNull())).count()\n",
    "                    col_info[\"empty_or_null_count\"] = empty_count\n",
    "                \n",
    "                distinct_count = df_sample.select(field.name).distinct().count()\n",
    "                col_info[\"distinct_count\"] = distinct_count\n",
    "                col_info[\"is_likely_categorical\"] = distinct_count < 50\n",
    "                \n",
    "                profile[\"columns\"].append(col_info)\n",
    "            \n",
    "            print(f\"‚úÖ Profile complete: {profile['row_count']} rows, {len(profile['columns'])} columns\")\n",
    "            return profile\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error profiling table: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "print(\"‚úÖ ETL Agent class loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a552c067-32c7-42cd-bc38-22f57f70abe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 1: Initialize Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd32d62-e95d-4235-b349-a66d293d07b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "agent = ETLAgent(spark)\n",
    "print(\"ü§ñ ETL Agent initialized\")\n",
    "print(f\"   Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc6aee5-0388-4f15-9204-71065717c735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Discover Available Tables\n",
    "# MAGIC \n",
    "# MAGIC The agent will scan Unity Catalog and find all available tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44133e13-9c4f-4465-8c64-3d5c3a48e962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# Discover all tables in prep_databricks catalog\n",
    "sources = agent.discover_sources(catalog=\"prep_databricks\", schema=\"default\")\n",
    "\n",
    "print(f\"\\nüìã Discovered {len(sources)} tables:\")\n",
    "for source in sources:\n",
    "    print(f\"  ‚Ä¢ {source['full_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc2dc66-4e29-459b-9385-994f7f26218c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Profile a Table\n",
    "# MAGIC \n",
    "# MAGIC Deep dive into the game_pass_games table to understand:\n",
    "# MAGIC * Row count and column types\n",
    "# MAGIC * Null percentages\n",
    "# MAGIC * Distinct values (for categorical detection)\n",
    "# MAGIC * Data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d165a88-e972-42a8-a518-ec70cfd59efe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# Profile the game_pass_games table\n",
    "table_to_profile = \"prep_databricks.default.game_pass_games\"\n",
    "profile = agent.profile_table(table_to_profile)\n",
    "\n",
    "print(f\"\\nüìä Profile Summary for {table_to_profile}:\")\n",
    "print(f\"  Total Rows: {profile['row_count']:,}\")\n",
    "print(f\"  Total Columns: {len(profile['columns'])}\")\n",
    "print(f\"  Schema: {profile['schema']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08f7200-5c7b-4114-b61d-62124dbab1bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 4: Analyze Column Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa27c967-9964-4f67-9c6e-ce2ebe78ee00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\nüìã Column Analysis:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for col in profile['columns']:\n",
    "    print(f\"\\nüîπ {col['name']} ({col['type']})\")\n",
    "    print(f\"   Nullable: {col['nullable']}\")\n",
    "    print(f\"   Null %: {col['null_percentage']}%\")\n",
    "    print(f\"   Distinct values: {col['distinct_count']}\")\n",
    "    \n",
    "    if col.get('is_likely_categorical'):\n",
    "        print(f\"   ‚ö†Ô∏è  Likely categorical - consider encoding for ML\")\n",
    "    \n",
    "    if col.get('null_percentage', 0) > 50:\n",
    "        print(f\"   ‚ö†Ô∏è  HIGH NULL PERCENTAGE - consider dropping or imputing\")\n",
    "    \n",
    "    if col.get('empty_or_null_count'):\n",
    "        print(f\"   Empty/Null count: {col['empty_or_null_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c089fd-ccd4-4f16-ae70-7b80e1af4ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 5: View Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1402d5-cc5f-4038-a1ab-7a0c0438a2b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "display(spark.table(table_to_profile).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ae51b8a-3137-4964-ab81-a688743570ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Key Insights\n",
    "# MAGIC \n",
    "# MAGIC Based on the profiling, the agent can now:\n",
    "# MAGIC * Identify which columns need cleaning (trimming, null handling)\n",
    "# MAGIC * Detect categorical columns for encoding\n",
    "# MAGIC * Suggest appropriate transformations\n",
    "# MAGIC * Generate optimized ETL code\n",
    "# MAGIC \n",
    "# MAGIC **Next:** Proceed to `02_generate_pipeline` to auto-generate ETL code!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Analyst Assistant Agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
